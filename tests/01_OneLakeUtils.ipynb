{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"fabric_remote_tools: OneLake CRUD Operations\"\n",
    "author: \"Renan Peres\"\n",
    "date: \"July 01, 2024\"\n",
    "format:\n",
    "    html:\n",
    "        theme: \n",
    "            light: flatly\n",
    "            dark: darkly\n",
    "        highlight-style: dracula\n",
    "        code-fold: show\n",
    "        code-tools: \n",
    "            source: true\n",
    "            toggle: true\n",
    "            # caption: none\n",
    "        code-copy: true\n",
    "        code-overflow: wrap\n",
    "        smooth-scroll: true       \n",
    "        page-layout: full\n",
    "        toc: true\n",
    "        toc-location: left\n",
    "        lightbox: true\n",
    "    ipynb:\n",
    "        output-file: output-file: \"fabric_remote_tools-OneLake-CRUD-Operations.ipynb\"\n",
    "        highlight-style: dracula\n",
    "        code-fold: show\n",
    "    pdf: default\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Install Package**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install https://github.com/renan-peres/fabric-remote-tools/raw/main/fabric_remote_tools-0.1.1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Modules & Authenticate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fabric_remote_tools import FabricAuth, OneLakeUtils\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Load Fabric Environmet Variables (.env File)\n",
    "account_name = os.getenv(\"ACCOUNT_NAME\")\n",
    "workspace_id = os.getenv(\"WORKSPACE_ID\")\n",
    "lakehouse_id = os.getenv(\"LAKEHOUSE_ID\")\n",
    "\n",
    "# Get Authentication Token\n",
    "token = FabricAuth.get_service_principal_token()\n",
    "\n",
    "# Get File System Client\n",
    "file_system_client = FabricAuth.get_file_system_client(token, account_name, workspace_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Write to Lakehouse (Files/Tables)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Local Tables (Delta)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Table\n",
    "OneLakeUtils.write_to_lakehouse(\n",
    "    file_system_client=file_system_client,\n",
    "    lakehouse_id=lakehouse_id,\n",
    "    upload_from=\"local\",\n",
    "    source_path=\"../assets/data/Tables/venture_funding_deals_delta_partitioned\",\n",
    "    target_path=\"Tables/local_venture_funding_deals_delta_partitioned\"\n",
    ")\n",
    "\n",
    "# Multiple Tables in a Folder\n",
    "OneLakeUtils.write_to_lakehouse(\n",
    "    file_system_client=file_system_client,\n",
    "    lakehouse_id=lakehouse_id,\n",
    "    upload_from=\"local\",\n",
    "    source_path=\"../assets/data/Tables\",\n",
    "    target_path=\"Tables/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Local Files/Folders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whole Folder\n",
    "OneLakeUtils.write_to_lakehouse(\n",
    "    file_system_client=file_system_client,\n",
    "    lakehouse_id=lakehouse_id,\n",
    "    upload_from=\"local\",\n",
    "    source_path=\"../assets/data/Files\",\n",
    "    target_path=\"Files/\"\n",
    ")\n",
    "\n",
    "# Individual Subfolder inside a Folder\n",
    "OneLakeUtils.write_to_lakehouse(\n",
    "    file_system_client=file_system_client,\n",
    "    lakehouse_id=lakehouse_id,\n",
    "    upload_from=\"local\",\n",
    "    source_path=\"../assets/data/Files/Contoso\",\n",
    "    target_path=\"Files/Contoso\"\n",
    ")\n",
    "\n",
    "# Specific File in a Folder\n",
    "OneLakeUtils.write_to_lakehouse(\n",
    "    file_system_client=file_system_client,\n",
    "    lakehouse_id=lakehouse_id,\n",
    "    upload_from=\"local\",\n",
    "    source_path=\"../assets/data/Files/Contoso/contoso_sales.csv\",\n",
    "    target_path=\"Files/Contoso/contoso_sales.csv\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **GitHub (Public Repo)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whole GitHub repository\n",
    "OneLakeUtils.write_to_lakehouse(\n",
    "    file_system_client=file_system_client,\n",
    "    lakehouse_id=lakehouse_id,\n",
    "    upload_from=\"github\",\n",
    "    source_path=\"https://github.com/renan-peres/Polars-Cookbook.git\",\n",
    "    target_path=\"Files/GitHub/Polars-Cookbook\"\n",
    ")\n",
    "\n",
    "# Single Table (Delta) in Repository\n",
    "OneLakeUtils.write_to_lakehouse(\n",
    "    file_system_client=file_system_client,\n",
    "    lakehouse_id=lakehouse_id,\n",
    "    upload_from=\"github\",\n",
    "    source_path=\"https://github.com/renan-peres/Polars-Cookbook.git\",\n",
    "    target_path=\"Tables/github_venture_funding_deals_delta\",\n",
    "    folder_path=\"data/venture_funding_deals_delta\"\n",
    ")\n",
    "\n",
    "# Specific folder from GitHub repository\n",
    "OneLakeUtils.write_to_lakehouse(\n",
    "    file_system_client=file_system_client,\n",
    "    lakehouse_id=lakehouse_id,\n",
    "    upload_from=\"github\",\n",
    "    source_path=\"https://github.com/renan-peres/Polars-Cookbook.git\",\n",
    "    target_path=\"Files/GitHub/data\",\n",
    "    folder_path=\"data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **GitHub (Private Repo)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_token = os.getenv(\"GH_PERSONAL_ACCESS_TOKEN\")\n",
    "github_username = os.getenv(\"GH_USERNAME\")\n",
    "gh_repo_name = os.getenv(\"GH_REPO_NAME\")\n",
    "\n",
    "# Whole GitHub private repository\n",
    "OneLakeUtils.write_to_lakehouse(\n",
    "    file_system_client=file_system_client,\n",
    "    lakehouse_id=lakehouse_id,\n",
    "    upload_from=\"github_private\",\n",
    "    github_token=github_token,\n",
    "    github_username=github_username,\n",
    "    repo_name=gh_repo_name,\n",
    "    target_path=f\"Files/GitHub/{gh_repo_name}\"\n",
    ")\n",
    "\n",
    "# Specific folder from GitHub private repository\n",
    "OneLakeUtils.write_to_lakehouse(\n",
    "    file_system_client=file_system_client,\n",
    "    lakehouse_id=lakehouse_id,\n",
    "    upload_from=\"github_private\",\n",
    "    github_token=github_token,\n",
    "    github_username=github_username,\n",
    "    repo_name=gh_repo_name,\n",
    "    target_path=\"Files/GitHub/data\",\n",
    "    folder_path=\"data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Azure DevOps (Private Repo)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "organization_url = os.getenv(\"ADO_ORGANIZATIONAL_URL\")\n",
    "personal_access_token = os.getenv(\"ADO_PERSONAL_ACCESS_TOKEN\")\n",
    "project_name = os.getenv(\"ADO_PROJECT_NAME\")\n",
    "repo_name = os.getenv(\"ADO_REPO_NAME\")\n",
    "\n",
    "# Whole Azure DevOps repository\n",
    "OneLakeUtils.write_to_lakehouse(\n",
    "    file_system_client=file_system_client,\n",
    "    lakehouse_id=lakehouse_id,\n",
    "    upload_from=\"azure_devops\",\n",
    "    project_name=project_name,\n",
    "    repo_name=repo_name,\n",
    "    organization_url=organization_url,\n",
    "    personal_access_token=personal_access_token,\n",
    "    target_path=f\"Files/AzureDevOps/{repo_name}\",\n",
    ")\n",
    "\n",
    "# Specific folder from Azure DevOps repository\n",
    "OneLakeUtils.write_to_lakehouse(\n",
    "    file_system_client=file_system_client,\n",
    "    lakehouse_id=lakehouse_id,\n",
    "    upload_from=\"azure_devops\",\n",
    "    project_name=project_name,\n",
    "    repo_name=repo_name,\n",
    "    organization_url=organization_url,\n",
    "    personal_access_token=personal_access_token,\n",
    "    target_path=\"Files/AzureDevOps/data\",\n",
    "    folder_path=\"/data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **List Items from Lakehouse (Files/Tables)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List All Items in Lakehouse\n",
    "OneLakeUtils.list_items(\n",
    "    file_system_client=file_system_client\n",
    "    ,lakehouse_id=lakehouse_id\n",
    "    ,target_directory_path=\"Tables\" # Tables or Files\n",
    "    #  ,print_output= True # Optional\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DeltaLake Table Operations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Read Table from Lakehouse into a DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌──────────────────────┬────────────────┬──────────────────────┬───┬──────────────────────┬───────────────┐\n",
       "│       Company        │     Amount     │    Lead investors    │ … │       Industry       │ Date reported │\n",
       "│       varchar        │    varchar     │       varchar        │   │       varchar        │    varchar    │\n",
       "├──────────────────────┼────────────────┼──────────────────────┼───┼──────────────────────┼───────────────┤\n",
       "│ Stripe               │ $6,500,000,000 │ n/a                  │ … │ Fintech              │ 3/15/23       │\n",
       "│ Inflection AI        │ $1,300,000,000 │ Microsoft, Reid Ho…  │ … │ Artificial intelli…  │ 6/29/23       │\n",
       "│ Anthropic            │ $1,250,000,000 │ Amazon               │ … │ Artificial intelli…  │ 9/25/23       │\n",
       "│ Lessen               │ $500,000,000   │ n/a                  │ … │ Real estate          │ 1/11/23       │\n",
       "│ Databricks           │ $500,000,000   │ funds and accounts…  │ … │ Data                 │ 9/14/23       │\n",
       "│ Anthropic            │ $450,000,000   │ Spark Capital        │ … │ Artificial intelli…  │ 5/23/23       │\n",
       "│ Adept AI             │ $350,000,000   │ General Catalyst, …  │ … │ Artificial intelli…  │ 3/14/23       │\n",
       "│ Axiom Space          │ $350,000,000   │ Aljazira Capital, …  │ … │ Space tech           │ 8/21/23       │\n",
       "│ Zipline              │ $330,000,000   │ n/a                  │ … │ Drones               │ 4/28/23       │\n",
       "│ Our Next Energy      │ $300,000,000   │ Franklin Templeton…  │ … │ Energy               │ 2/1/23        │\n",
       "│        ·             │      ·         │  ·                   │ · │   ·                  │   ·           │\n",
       "│        ·             │      ·         │  ·                   │ · │   ·                  │   ·           │\n",
       "│        ·             │      ·         │  ·                   │ · │   ·                  │   ·           │\n",
       "│ Eikon Therapeutics   │ $106,000,000   │ n/a                  │ … │ Biotech              │ 6/1/23        │\n",
       "│ R-Zero               │ $105,000,000   │ CDPQ                 │ … │ Biotech              │ 2/14/23       │\n",
       "│ CG Oncology          │ $105,000,000   │ Foresite Capital, …  │ … │ Biotech              │ 8/2/23        │\n",
       "│ Synthekine           │ $100,000,000   │ The Column Group     │ … │ Biotech              │ 1/6/23        │\n",
       "│ Paratus Sciences     │ $100,000,000   │ Polaris Partners, …  │ … │ Biotech              │ 2/27/23       │\n",
       "│ Rapport Therapeutics │ $100,000,000   │ Third Rock Venture…  │ … │ Biotech              │ 3/7/23        │\n",
       "│ Boundless Bio        │ $100,000,000   │ Leaps by Bayer, RA…  │ … │ Biotech              │ 5/16/23       │\n",
       "│ Ray Therapeutics     │ $100,000,000   │ Novo Holdings A/S    │ … │ Biotech              │ 5/16/23       │\n",
       "│ Acepodia             │ $100,000,000   │ Digital Mobile Ven…  │ … │ Biotech              │ 6/6/23        │\n",
       "│ Inceptive            │ $100,000,000   │ NVentures, Andrees…  │ … │ Biotech              │ 9/7/23        │\n",
       "├──────────────────────┴────────────────┴──────────────────────┴───┴──────────────────────┴───────────────┤\n",
       "│ 171 rows (20 shown)                                                                 6 columns (5 shown) │\n",
       "└─────────────────────────────────────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fabric_remote_tools import FabricAuth, OneLakeUtils\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() # Load environment variables from .env file\n",
    "\n",
    "# Authenticate and obtain access token\n",
    "file_system_client = FabricAuth().get_client_secret_token()\n",
    "\n",
    "# Read Table from Lakehouse into Dataframe\n",
    "workspace_name = os.getenv(\"WORKSPACE_NAME\")\n",
    "lakehouse_name = os.getenv(\"LAKEHOUSE_NAME\")\n",
    "table_name = \"Tables/venture_funding_deals_delta\"\n",
    "table_path = f\"abfss://{workspace_name}@onelake.dfs.fabric.microsoft.com/{lakehouse_name}.Lakehouse/{table_name}\"\n",
    "\n",
    "df = OneLakeUtils().read_deltalake(\n",
    "    file_system_client=file_system_client,\n",
    "    table_path=table_path,\n",
    "    engine='duckdb',  # Supported options: 'duckdb', 'polars'\n",
    "    # version=11,  # Optional: specify the version to read\n",
    "    # row_limit=10  # Optional\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Query DataFrame with DuckDB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install magic_duckdb --upgrade --quiet\n",
    "%load_ext magic_duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>column_type</th>\n",
       "      <th>null</th>\n",
       "      <th>key</th>\n",
       "      <th>default</th>\n",
       "      <th>extra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Company</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amount</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lead investors</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Valuation</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Industry</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Date reported</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      column_name column_type null   key default extra\n",
       "0         Company     VARCHAR  YES  None    None  None\n",
       "1          Amount     VARCHAR  YES  None    None  None\n",
       "2  Lead investors     VARCHAR  YES  None    None  None\n",
       "3       Valuation     VARCHAR  YES  None    None  None\n",
       "4        Industry     VARCHAR  YES  None    None  None\n",
       "5   Date reported     VARCHAR  YES  None    None  None"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%dql\n",
    "PRAGMA disable_print_progress_bar;\n",
    "SUMMARIZE df;\n",
    "DESCRIBE SELECT * FROM df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Count\n",
       "0     20"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%dql\n",
    "CREATE OR REPLACE TABLE df_tranf AS \n",
    "    SELECT *\n",
    "    FROM df\n",
    "    LIMIT 20;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Write DataFrame to Lakehouse as a Delta Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deltalake.writer import write_deltalake\n",
    "import duckdb\n",
    "import pyarrow\n",
    "import polars as pl\n",
    "\n",
    "# Write DataFrame to Lakehouse\n",
    "write_deltalake(\n",
    "    table_or_uri=table_path\n",
    "    ,storage_options=file_system_client\n",
    "    # ,data=df.to_arrow() # Polars DF\n",
    "    ,data=duckdb.sql(\"SELECT * FROM df_tranf\").arrow() # DuckDB (arrow DF)\n",
    "    ,mode=\"append\" # Supported options: 'append', 'overwrite'\n",
    "    ,engine=\"rust\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **DESCRIBE HISTORY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>operation</th>\n",
       "      <th>operationParameters</th>\n",
       "      <th>readVersion</th>\n",
       "      <th>isolationLevel</th>\n",
       "      <th>isBlindAppend</th>\n",
       "      <th>engineInfo</th>\n",
       "      <th>txnId</th>\n",
       "      <th>operationMetrics</th>\n",
       "      <th>version</th>\n",
       "      <th>tags</th>\n",
       "      <th>clientVersion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-07-02 19:52:55.075</td>\n",
       "      <td>VACUUM END</td>\n",
       "      <td>{'status': 'COMPLETED'}</td>\n",
       "      <td>6.0</td>\n",
       "      <td>SnapshotIsolation</td>\n",
       "      <td>True</td>\n",
       "      <td>Apache-Spark/3.4.1.5.3.20240528.1 Delta-Lake/2...</td>\n",
       "      <td>4ebe44b4-d133-4e06-b659-1deb812dee77</td>\n",
       "      <td>{'numDeletedFiles': '0', 'numVacuumedDirectori...</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-07-02 19:52:53.249</td>\n",
       "      <td>VACUUM START</td>\n",
       "      <td>{'defaultRetentionMillis': 604800000, 'retenti...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>SnapshotIsolation</td>\n",
       "      <td>True</td>\n",
       "      <td>Apache-Spark/3.4.1.5.3.20240528.1 Delta-Lake/2...</td>\n",
       "      <td>60f1f933-7958-47c1-bf8e-46031dac75f2</td>\n",
       "      <td>{'numFilesToDelete': '0', 'sizeOfDataToDelete'...</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-07-02 19:51:06.952</td>\n",
       "      <td>VACUUM END</td>\n",
       "      <td>{'status': 'COMPLETED'}</td>\n",
       "      <td>4.0</td>\n",
       "      <td>SnapshotIsolation</td>\n",
       "      <td>True</td>\n",
       "      <td>Apache-Spark/3.4.1.5.3.20240528.1 Delta-Lake/2...</td>\n",
       "      <td>a63cdab8-c767-4b50-b4b2-7da6f632754b</td>\n",
       "      <td>{'numDeletedFiles': '0', 'numVacuumedDirectori...</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-07-02 19:51:05.207</td>\n",
       "      <td>VACUUM START</td>\n",
       "      <td>{'retentionCheckEnabled': True, 'defaultRetent...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>SnapshotIsolation</td>\n",
       "      <td>True</td>\n",
       "      <td>Apache-Spark/3.4.1.5.3.20240528.1 Delta-Lake/2...</td>\n",
       "      <td>baa959c3-6e72-4ccd-b5c2-59c98aa92414</td>\n",
       "      <td>{'numFilesToDelete': '0', 'sizeOfDataToDelete'...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-07-02 19:49:43.671</td>\n",
       "      <td>OPTIMIZE</td>\n",
       "      <td>{'vorder': True, 'zOrderBy': '[]', 'auto': Fal...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>SnapshotIsolation</td>\n",
       "      <td>False</td>\n",
       "      <td>Apache-Spark/3.4.1.5.3.20240528.1 Delta-Lake/2...</td>\n",
       "      <td>5f647a7a-88b1-4442-8db0-5d38f6bb2a85</td>\n",
       "      <td>{'maxFileSize': '10416', 'minFileSize': '10416...</td>\n",
       "      <td>3</td>\n",
       "      <td>{'VORDER': 'true'}</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp     operation  \\\n",
       "0 2024-07-02 19:52:55.075    VACUUM END   \n",
       "1 2024-07-02 19:52:53.249  VACUUM START   \n",
       "2 2024-07-02 19:51:06.952    VACUUM END   \n",
       "3 2024-07-02 19:51:05.207  VACUUM START   \n",
       "4 2024-07-02 19:49:43.671      OPTIMIZE   \n",
       "\n",
       "                                 operationParameters  readVersion  \\\n",
       "0                            {'status': 'COMPLETED'}          6.0   \n",
       "1  {'defaultRetentionMillis': 604800000, 'retenti...          5.0   \n",
       "2                            {'status': 'COMPLETED'}          4.0   \n",
       "3  {'retentionCheckEnabled': True, 'defaultRetent...          3.0   \n",
       "4  {'vorder': True, 'zOrderBy': '[]', 'auto': Fal...          2.0   \n",
       "\n",
       "      isolationLevel isBlindAppend  \\\n",
       "0  SnapshotIsolation          True   \n",
       "1  SnapshotIsolation          True   \n",
       "2  SnapshotIsolation          True   \n",
       "3  SnapshotIsolation          True   \n",
       "4  SnapshotIsolation         False   \n",
       "\n",
       "                                          engineInfo  \\\n",
       "0  Apache-Spark/3.4.1.5.3.20240528.1 Delta-Lake/2...   \n",
       "1  Apache-Spark/3.4.1.5.3.20240528.1 Delta-Lake/2...   \n",
       "2  Apache-Spark/3.4.1.5.3.20240528.1 Delta-Lake/2...   \n",
       "3  Apache-Spark/3.4.1.5.3.20240528.1 Delta-Lake/2...   \n",
       "4  Apache-Spark/3.4.1.5.3.20240528.1 Delta-Lake/2...   \n",
       "\n",
       "                                  txnId  \\\n",
       "0  4ebe44b4-d133-4e06-b659-1deb812dee77   \n",
       "1  60f1f933-7958-47c1-bf8e-46031dac75f2   \n",
       "2  a63cdab8-c767-4b50-b4b2-7da6f632754b   \n",
       "3  baa959c3-6e72-4ccd-b5c2-59c98aa92414   \n",
       "4  5f647a7a-88b1-4442-8db0-5d38f6bb2a85   \n",
       "\n",
       "                                    operationMetrics  version  \\\n",
       "0  {'numDeletedFiles': '0', 'numVacuumedDirectori...        7   \n",
       "1  {'numFilesToDelete': '0', 'sizeOfDataToDelete'...        6   \n",
       "2  {'numDeletedFiles': '0', 'numVacuumedDirectori...        5   \n",
       "3  {'numFilesToDelete': '0', 'sizeOfDataToDelete'...        4   \n",
       "4  {'maxFileSize': '10416', 'minFileSize': '10416...        3   \n",
       "\n",
       "                 tags clientVersion  \n",
       "0                 NaN           NaN  \n",
       "1                 NaN           NaN  \n",
       "2                 NaN           NaN  \n",
       "3                 NaN           NaN  \n",
       "4  {'VORDER': 'true'}           NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deltalake import DeltaTable\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the DeltaTable\n",
    "dt = DeltaTable(table_path)\n",
    "\n",
    "# Retrieve the full history of the DeltaTable\n",
    "history = dt.history()\n",
    "\n",
    "# Convert the history list to a pandas DataFrame\n",
    "history_df = pd.DataFrame(history)\n",
    "\n",
    "# Parse the timestamp column\n",
    "history_df['timestamp'] = pd.to_datetime(history_df['timestamp'], unit='ms')\n",
    "\n",
    "# Display the DataFrame, sorted by version in descending order\n",
    "display(history_df.sort_values(by='version', ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Download Items from Lakehouse (Files/Tables)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables\n",
    "OneLakeUtils.download_from_lakehouse(\n",
    "    file_system_client=file_system_client\n",
    "    ,lakehouse_id=lakehouse_id\n",
    "    # ,target_file_path=\"Tables/venture_funding_deals\" # Single Table\n",
    "    ,target_file_path=\"Tables/\" # All Tables\n",
    ")\n",
    "\n",
    "# Files\n",
    "OneLakeUtils.download_from_lakehouse(\n",
    "    file_system_client=file_system_client\n",
    "    ,lakehouse_id=lakehouse_id\n",
    "    # ,target_file_path=\"Files/Contoso/contoso_sales.csv\" # Single File\n",
    "    # ,target_file_path=\"Files/Contoso/\" # Subfolder\n",
    "    ,target_file_path=\"Files/\" # All Subfolders & Files\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Delete Items from Lakehouse (Files/Tables)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables\n",
    "OneLakeUtils.delete_file(\n",
    "    file_system_client=file_system_client\n",
    "    ,lakehouse_id=lakehouse_id\n",
    "    # ,lakehouse_dir_path=\"Tables/venture_funding_deals_delta\" # Single Table\n",
    "    ,lakehouse_dir_path=\"Tables/\" # All Tables\n",
    ")\n",
    "\n",
    "# Files\n",
    "OneLakeUtils.delete_file(\n",
    "    file_system_client=file_system_client \n",
    "    ,lakehouse_id=lakehouse_id\n",
    "    # ,lakehouse_dir_path=\"Files/Contoso/contoso_sales.csv\" # Single File\n",
    "    # ,lakehouse_dir_path=\"Files/Contoso\" # Subfolder\n",
    "    ,lakehouse_dir_path=\"Files/\" # All Subfolders & Files\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
