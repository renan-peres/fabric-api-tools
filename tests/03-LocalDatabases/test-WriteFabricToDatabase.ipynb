{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Install/Update Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Download Delta Tables from Lakehouse and Write to DuckDB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run func-OneLakeStorageOperations.py\n",
    "%run func-WriteFabricToDuckDb.py\n",
    "import os\n",
    "import polars as pl\n",
    "import duckdb\n",
    "\n",
    "# Create an instance of AzureStorageOperations\n",
    "azure_ops = AzureStorageOperations()\n",
    "\n",
    "# Get Authentication Token\n",
    "token = azure_ops.get_authentication_token()\n",
    "file_system_client = azure_ops.get_file_system_client(token)\n",
    "\n",
    "# Connect to DuckDB\n",
    "# conn = duckdb.connect('md:') # MotherDuck\n",
    "# conn = duckdb.connect('Tables/data.db') # Local DuckDB Database\n",
    "conn = duckdb.connect('../Tables/LH_Gold.db') # New DuckDB Database\n",
    "\n",
    "# Select Database (If MotherDuck)\n",
    "# conn.sql(f'CREATE OR REPLACE DATABASE LH_DeltaLake;')\n",
    "# conn.sql(f'CREATE DATABASE IF NOT EXISTS LH_Bronze;')\n",
    "# conn.sql(f'USE LH_Bronze;')\n",
    "\n",
    "# Create Schema\n",
    "# dt_today = datetime.today()\n",
    "# dt_string = 'dbo_' + dt_today.strftime(\"%Y_%m_%d\")\n",
    "# conn.sql(f'CREATE OR REPLACE SCHEMA {dt_string}')\n",
    "\n",
    "# Get the filtered subdirectory names for \"Tables\"\n",
    "filtered_tables = azure_ops.list_items(file_system_client=file_system_client, target_directory_path=\"Tables\")\n",
    "# partial_match = ['fact', 'dim'] # Partial match strings\n",
    "# filtered_tables = [table for table in filtered_tables if any(keyword in table for keyword in partial_match)] # Filter for Conatains\n",
    "# filtered_tables = [table for table in filtered_tables if any(table.startswith(keyword) for keyword in partial_match)] # Filter for Starts With\n",
    "\n",
    "# Prepare a list of SQL statements for batch execution\n",
    "sql_statements = []\n",
    "\n",
    "for table in filtered_tables:\n",
    "    try:\n",
    "        # Read the delta file from the lakehouse\n",
    "        local_file_name = azure_ops.download_from_lakehouse(\n",
    "            file_system_client=file_system_client,\n",
    "            target_file_path=f'Tables/{table}',\n",
    "            is_delta=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Local file path for table {table}: {local_file_name}\")\n",
    "        \n",
    "        if not os.path.exists(local_file_name):\n",
    "            print(f\"[E] File path does not exist: {local_file_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Prepare SQL statement for batch execution\n",
    "        sql_statements.append(f\"\"\"\n",
    "        DROP TABLE IF EXISTS {table};\n",
    "        CREATE TABLE {table} AS SELECT * FROM parquet_scan('{local_file_name}');\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\"[I] Table {table} prepared for creation/replacement in DuckDB.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[E] Error processing table {table}: {e}\")\n",
    "    finally:\n",
    "        # Clean up downloaded files immediately\n",
    "        if local_file_name and os.path.exists(local_file_name):\n",
    "            azure_ops.delete_local_path(local_file_name)\n",
    "\n",
    "# Execute all SQL statements in a single transaction\n",
    "with conn.cursor() as cursor:\n",
    "    cursor.execute(\"BEGIN TRANSACTION;\")\n",
    "    for statement in sql_statements:\n",
    "        cursor.execute(statement)\n",
    "    cursor.execute(\"COMMIT;\")\n",
    "\n",
    "print(\"[I] All tables created/replaced in DuckDB.\")\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Write DuckDB to SQL Server**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Polars: write_database()**\n",
    "**Documentation: [polars.DataFrame.write_database()](https://docs.pola.rs/api/python/stable/reference/api/polars.DataFrame.write_database.html)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../main.py\n",
    "import duckdb\n",
    "import os\n",
    "import polars as pl\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import OperationalError\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define connection parameters\n",
    "server = os.getenv(\"server\")\n",
    "user = os.getenv(\"user\")\n",
    "password = os.getenv(\"password\")\n",
    "database = \"LH_GOLD\"\n",
    "\n",
    "# Create the SQL Server URI connection string\n",
    "sql_server_uri = f\"mssql+pyodbc://{user}:{password}@{server}/{database}?driver=ODBC+Driver+17+for+SQL+Server\"\n",
    "\n",
    "def execute_non_transactional_query(query):\n",
    "    try:\n",
    "        engine = create_engine(f\"mssql+pyodbc://{user}:{password}@{server}/master?driver=ODBC+Driver+17+for+SQL+Server\")\n",
    "        with engine.connect() as conn:\n",
    "            conn.execute(text(query).execution_options(autocommit=True))  # Ensure the query is executed outside of a transaction\n",
    "        print(f\"Query executed successfully: {query}\")\n",
    "    except OperationalError as e:\n",
    "        print(f\"OperationalError: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing query: {e}\")\n",
    "\n",
    "# Drop the LH_GOLD database if it exists\n",
    "drop_db_query = \"IF EXISTS (SELECT name FROM sys.databases WHERE name = 'LH_GOLD') DROP DATABASE LH_GOLD;\"\n",
    "execute_non_transactional_query(drop_db_query)\n",
    "\n",
    "# Create the LH_GOLD database\n",
    "create_db_query = \"CREATE DATABASE LH_GOLD;\"\n",
    "execute_non_transactional_query(create_db_query)\n",
    "\n",
    "# Define a function to process and transfer each table\n",
    "def process_table(table, duckdb_connection, sql_server_uri, progress_bar):\n",
    "    try:\n",
    "        # Establish connection to DuckDB\n",
    "        con_duckdb = duckdb.connect(duckdb_connection)\n",
    "        \n",
    "        # Fetch data from DuckDB table\n",
    "        df_arrow = con_duckdb.execute(f'SELECT * FROM {table}').arrow()\n",
    "        df = pl.from_arrow(df_arrow)\n",
    "        \n",
    "        # Optionally transform the DataFrame\n",
    "        df = df.with_columns(pl.col(\"*\").cast(pl.Utf8))\n",
    "\n",
    "        # Use Polars' write_database method to write the DataFrame to SQL Server\n",
    "        df.write_database(table, connection=sql_server_uri, if_table_exists='replace')\n",
    "\n",
    "        print(f\"Data successfully written to table {table} in SQL Server.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error while processing table {table}: {e}\")\n",
    "    finally:\n",
    "        con_duckdb.close()\n",
    "        # Update the progress bar\n",
    "        progress_bar.update(1)\n",
    "\n",
    "# Establish connection to DuckDB\n",
    "duckdb_connection = '../Tables/LH_Gold.db'\n",
    "con_duckdb = duckdb.connect(duckdb_connection)\n",
    "\n",
    "# Fetch tables from DuckDB\n",
    "tables = con_duckdb.execute(\"SHOW TABLES\").fetchdf()\n",
    "table_list = tables['name'].tolist()\n",
    "\n",
    "# Create a tqdm progress bar\n",
    "with tqdm(total=len(table_list), desc=\"Processing tables\", unit=\"table\") as progress_bar:\n",
    "    # Use ThreadPoolExecutor to process tables in parallel\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        futures = [executor.submit(process_table, table, duckdb_connection, sql_server_uri, progress_bar) for table in table_list]\n",
    "\n",
    "        # Process each future as it completes\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"Error occurred: {e}\")\n",
    "\n",
    "print(\"All operations completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Turbodbc (Faster) - Conda Kernel**\n",
    "**Documentation: [Turbodbc Website](https://turbodbc.readthedocs.io/en/latest/pages/introduction.html)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<u>SQL Server</u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DATABASE: 'LH_GOLD' exists.\n",
      "TABLE: 'dim_budget_2024' checked/created successfully in sqlserver.\n",
      "TABLE: 'dim_productcategory_gold' checked/created successfully in sqlserver.\n",
      "TABLE: 'dim_plant_gold' checked/created successfully in sqlserver.\n",
      "INSERT 0.00s: '6 rows' successfully written to table 'dim_productcategory_gold' in sqlserver.\n",
      "TABLE: 'dim_coa_gold' checked/created successfully in sqlserver.\n",
      "TABLE: 'dim_salesperson_gold' checked/created successfully in sqlserver.\n",
      "TABLE: 'dim_date_gold' checked/created successfully in sqlserver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully: IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = 'dim_budget_2024') CREATE TABLE dim_budget_2024 ([GL_ACCOUNT] NVARCHAR(MAX), [DESCRIPTION] NVARCHAR(MAX), [AMOUNT] NVARCHAR(MAX), [YEAR] NVARCHAR(MAX), [MONTH] NVARCHAR(MAX), [SEGMENT_1] NVARCHAR(MAX), [SEGMENT_2] NVARCHAR(MAX))\n",
      "Query executed successfully: IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = 'dim_productcategory_gold') CREATE TABLE dim_productcategory_gold ([PRODUCT_CATEGORY] NVARCHAR(MAX), [INDEX] NVARCHAR(MAX))\n",
      "Query executed successfully: IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = 'dim_plant_gold') CREATE TABLE dim_plant_gold ([PLANT_NO] NVARCHAR(MAX), [PLANT_NAME] NVARCHAR(MAX), [DESCRIPTION] NVARCHAR(MAX), [ADDRESS_1] NVARCHAR(MAX), [ADDRESS_2] NVARCHAR(MAX), [PHONE_NO] NVARCHAR(MAX), [PLANT_SCREEN_ID] NVARCHAR(MAX), [LAST_CHANGE_DATETIME] NVARCHAR(MAX), [LAST_CHANGE_USER] NVARCHAR(MAX), [ACTIVE_FLAG] NVARCHAR(MAX))\n",
      "Query executed successfully: IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = 'dim_coa_gold') CREATE TABLE dim_coa_gold ([ACCOUNT_INDEX] NVARCHAR(MAX), [GL_ACCOUNT] NVARCHAR(MAX), [ACTIVE_FLAG] NVARCHAR(MAX), [DESCRIPTION] NVARCHAR(MAX), [GL_GROUP] NVARCHAR(MAX), [GROUP_DESCRIPTION] NVARCHAR(MAX), [ACCOUNT_CLASS] NVARCHAR(MAX), [SEGMENT_1] NVARCHAR(MAX), [SEGMENT_2] NVARCHAR(MAX), [ACCOUNT_TYPE] NVARCHAR(MAX), [DR_CR_FLAG] NVARCHAR(MAX), [PRODUCT_CATEGORY] NVARCHAR(MAX), [UNIT_OF_MEASURE] NVARCHAR(MAX), [EVENT_LINK] NVARCHAR(MAX), [LAST_CHANGE_DATETIME] NVARCHAR(MAX), [LAST_CHANGE_USER] NVARCHAR(MAX))\n",
      "Query executed successfully: IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = 'dim_salesperson_gold') CREATE TABLE dim_salesperson_gold ([CLASS_NO] NVARCHAR(MAX), [NAME] NVARCHAR(MAX), [JOB_TITLE] NVARCHAR(MAX), [PHONE_NO] NVARCHAR(MAX), [FAX_NO] NVARCHAR(MAX), [CELL_NO] NVARCHAR(MAX), [EMAIL] NVARCHAR(MAX), [LAST_CHANGE_DATETIME] NVARCHAR(MAX), [LAST_CHANGE_USER] NVARCHAR(MAX), [ACTIVE_FLAG] NVARCHAR(MAX))\n",
      "Query executed successfully: IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = 'dim_date_gold') CREATE TABLE dim_date_gold ([Date] NVARCHAR(MAX), [Year] NVARCHAR(MAX), [Quarter_Number] NVARCHAR(MAX), [Quarter] NVARCHAR(MAX), [Start_of_Quarter] NVARCHAR(MAX), [End_of_Quarter] NVARCHAR(MAX), [Quarter_and_Year] NVARCHAR(MAX), [Month] NVARCHAR(MAX), [Month_Name] NVARCHAR(MAX), [Month_Short] NVARCHAR(MAX), [Month_and_Year] NVARCHAR(MAX), [Week_Number] NVARCHAR(MAX), [Week_Name] NVARCHAR(MAX), [Week_and_Year] NVARCHAR(MAX), [Day_of_Week_Number] NVARCHAR(MAX), [Day_of_Week_Name] NVARCHAR(MAX))\n",
      "Query executed successfully: IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = 'dim_customers_gold') CREATE TABLE dim_customers_gold ([CUST_NO] NVARCHAR(MAX), [NAME] NVARCHAR(MAX), [LAST_SALE_DATE] NVARCHAR(MAX), [ADDRESS_1] NVARCHAR(MAX), [ADDRESS_2] NVARCHAR(MAX), [CITY] NVARCHAR(MAX), [STATE] NVARCHAR(MAX), [ZIP] NVARCHAR(MAX), [CONTACT] NVARCHAR(MAX), [CREDIT_LIMIT] NVARCHAR(MAX), [HIGH_CREDIT] NVARCHAR(MAX), [SALESPERSON] NVARCHAR(MAX), [LAST_PAY_DATE] NVARCHAR(MAX), [PAID_THIS_MONTH] NVARCHAR(MAX), [OLDEST_AR_DATE] NVARCHAR(MAX), [AR_BALANCE] NVARCHAR(MAX), [PHONE_NO] NVARCHAR(MAX), [FAX_NO] NVARCHAR(MAX), [LAST_CONTACT_DATE] NVARCHAR(MAX), [MTD_SALES] NVARCHAR(MAX), [YTD_SALES] NVARCHAR(MAX), [MTD_GROSS_PROFIT] NVARCHAR(MAX), [YTD_GROSS_PROFIT] NVARCHAR(MAX), [LAST_YEAR_SALES] NVARCHAR(MAX), [MTD_DISC_TAKEN] NVARCHAR(MAX), [YTD_DISC_TAKEN] NVARCHAR(MAX), [CREDIT_WARNING] NVARCHAR(MAX), [SEQUENCE_CODE] NVARCHAR(MAX), [TAX_ID] NVARCHAR(MAX), [EMAIL_STMT_STATUS] NVARCHAR(MAX), [LAST_CHANGE_DATETIME] NVARCHAR(MAX), [LAST_CHANGE_USER] NVARCHAR(MAX), [ACTIVE_FLAG] NVARCHAR(MAX))\n",
      "Query executed successfully: IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = 'dim_products_gold') CREATE TABLE dim_products_gold ([PRODUCT_CODE] NVARCHAR(MAX), [UNIT_OF_MEASURE] NVARCHAR(MAX), [DESCRIPTION_1] NVARCHAR(MAX), [DESCRIPTION_2] NVARCHAR(MAX), [LAST_SALE_DATE] NVARCHAR(MAX), [SALES_GL] NVARCHAR(MAX), [BASE_PRODUCT] NVARCHAR(MAX), [PRODUCT_CLASS] NVARCHAR(MAX), [PLANT_NO] NVARCHAR(MAX), [LAST_MONTH_QTY_SOLD] NVARCHAR(MAX), [MTD_QTY_SOLD] NVARCHAR(MAX), [YTD_QTY_SOLD] NVARCHAR(MAX), [MTD_SALES] NVARCHAR(MAX), [YTD_SALES] NVARCHAR(MAX), [YTD_GROSS_PROFIT] NVARCHAR(MAX), [LAST_YEAR_QTY_SOLD] NVARCHAR(MAX), [LAST_YEAR_SALES] NVARCHAR(MAX), [LIST_PRICE] NVARCHAR(MAX), [STANDARD_COST] NVARCHAR(MAX), [STD_PCT_DISC] NVARCHAR(MAX), [AUTO_QTY] NVARCHAR(MAX), [LAST_CHANGE_DATETIME] NVARCHAR(MAX), [LAST_CHANGE_USER] NVARCHAR(MAX), [ACTIVE_FLAG] NVARCHAR(MAX))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INSERT 0.14s: '4735 rows' successfully written to table 'dim_budget_2024' in sqlserver.\n",
      "TABLE: 'dim_customers_gold' checked/created successfully in sqlserver.\n",
      "INSERT 0.01s: '13 rows' successfully written to table 'dim_plant_gold' in sqlserver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully: IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = 'dim_unitofmeasure_gold') CREATE TABLE dim_unitofmeasure_gold ([INDEX] NVARCHAR(MAX), [UNIT_OF_MEASURE] NVARCHAR(MAX), [UNIT_NAME] NVARCHAR(MAX), [DESCRIPTION] NVARCHAR(MAX), [UMS_RATIO] NVARCHAR(MAX), [PRICE_ROUND_FACTOR] NVARCHAR(MAX), [LAST_CHANGE_DATETIME] NVARCHAR(MAX), [LAST_CHANGE_USER] NVARCHAR(MAX), [ACTIVE_FLAG] NVARCHAR(MAX))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TABLE: 'dim_products_gold' checked/created successfully in sqlserver.\n",
      "INSERT 0.06s: '1509 rows' successfully written to table 'dim_coa_gold' in sqlserver.\n",
      "INSERT 0.00s: '12 rows' successfully written to table 'dim_salesperson_gold' in sqlserver.\n",
      "TABLE: 'dim_unitofmeasure_gold' checked/created successfully in sqlserver.\n",
      "INSERT 0.30s: '3007 rows' successfully written to table 'dim_customers_gold' in sqlserver.\n",
      "INSERT 1.35s: '5844 rows' successfully written to table 'dim_date_gold' in sqlserver.\n",
      "INSERT 0.01s: '33 rows' successfully written to table 'dim_unitofmeasure_gold' in sqlserver.\n",
      "INSERT 1.07s: '28750 rows' successfully written to table 'dim_products_gold' in sqlserver.\n",
      "TABLE: 'fact_sales_gold' checked/created successfully in sqlserver.\n",
      "Processing large table 'fact_sales_gold' in 14 chunks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully: IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = 'fact_sales_gold') CREATE TABLE fact_sales_gold ([SESSION_NO] NVARCHAR(MAX), [CUST_NO] NVARCHAR(MAX), [INVOICE_NO] NVARCHAR(MAX), [INVOICE_DATE] NVARCHAR(MAX), [TICKET_NO] NVARCHAR(MAX), [TICKET_DATE] NVARCHAR(MAX), [SALESPERSON] NVARCHAR(MAX), [VOID_FLAG] NVARCHAR(MAX), [PRODUCT_CODE] NVARCHAR(MAX), [UNIT_OF_MEASURE] NVARCHAR(MAX), [PLANT_NO] NVARCHAR(MAX), [UNIT_PRICE] NVARCHAR(MAX), [LIST_PRICE] NVARCHAR(MAX), [QUOTE_PRICE] NVARCHAR(MAX), [UNIT_DISC] NVARCHAR(MAX), [QUOTE_DISC] NVARCHAR(MAX), [QTY_SOLD] NVARCHAR(MAX), [SALES_GL] NVARCHAR(MAX), [TOT_REVENUE] NVARCHAR(MAX), [TOT_COST_OF_SALES] NVARCHAR(MAX), [TOT_DISCOUNT] NVARCHAR(MAX), [TOT_GROSS_PROFIT] NVARCHAR(MAX))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c442db459bd47b882d9516d33eb5f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INSERT: Chunk of data written to table 'fact_sales_gold' in sqlserver.\n",
      "INSERT 3.75s: '100000 rows' successfully written to table 'fact_sales_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_sales_gold' in sqlserver.\n",
      "INSERT 3.46s: '100000 rows' successfully written to table 'fact_sales_gold' in sqlserver.\n",
      "TABLE: 'fact_gl_gold' checked/created successfully in sqlserver.\n",
      "Processing large table 'fact_gl_gold' in 34 chunks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully: IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = 'fact_gl_gold') CREATE TABLE fact_gl_gold ([REFRESHED_AT] NVARCHAR(MAX), [SESSION_NO] NVARCHAR(MAX), [TRANS_NO] NVARCHAR(MAX), [TRX_DATE] NVARCHAR(MAX), [SOURCE_CODE] NVARCHAR(MAX), [GL_ACCOUNT] NVARCHAR(MAX), [PLANT_NO] NVARCHAR(MAX), [GL_DESCRIPTION] NVARCHAR(MAX), [DEBIT_AMOUNT] NVARCHAR(MAX), [CREDIT_AMOUNT] NVARCHAR(MAX), [TRX_AMOUNT] NVARCHAR(MAX), [TRX_AMOUNT_INDEX] NVARCHAR(MAX), [TRX_AMOUNT_DR_CR] NVARCHAR(MAX), [DESCRIPTION_1] NVARCHAR(MAX), [DESCRIPTION_2] NVARCHAR(MAX), [YEAR] NVARCHAR(MAX), [MONTH] NVARCHAR(MAX), [LAST_CHANGE_DATETIME] NVARCHAR(MAX), [LAST_CHANGE_USER] NVARCHAR(MAX))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INSERT: Chunk of data written to table 'fact_sales_gold' in sqlserver.\n",
      "INSERT 6.67s: '100000 rows' successfully written to table 'fact_sales_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT 6.57s: '100000 rows' successfully written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_sales_gold' in sqlserver.\n",
      "INSERT 6.24s: '100000 rows' successfully written to table 'fact_sales_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT 6.12s: '100000 rows' successfully written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_sales_gold' in sqlserver.\n",
      "INSERT 6.12s: '100000 rows' successfully written to table 'fact_sales_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT 6.28s: '100000 rows' successfully written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_sales_gold' in sqlserver.\n",
      "INSERT 6.40s: '100000 rows' successfully written to table 'fact_sales_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT 6.32s: '100000 rows' successfully written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_sales_gold' in sqlserver.\n",
      "INSERT 6.38s: '100000 rows' successfully written to table 'fact_sales_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT 6.48s: '100000 rows' successfully written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_sales_gold' in sqlserver.\n",
      "INSERT 6.74s: '100000 rows' successfully written to table 'fact_sales_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT 6.57s: '100000 rows' successfully written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_sales_gold' in sqlserver.\n",
      "INSERT 6.28s: '100000 rows' successfully written to table 'fact_sales_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT 6.36s: '100000 rows' successfully written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_sales_gold' in sqlserver.\n",
      "INSERT 6.53s: '100000 rows' successfully written to table 'fact_sales_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT 6.43s: '100000 rows' successfully written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_sales_gold' in sqlserver.\n",
      "INSERT 6.78s: '100000 rows' successfully written to table 'fact_sales_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT 6.76s: '100000 rows' successfully written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_sales_gold' in sqlserver.\n",
      "INSERT 7.16s: '100000 rows' successfully written to table 'fact_sales_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT 7.23s: '100000 rows' successfully written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_sales_gold' in sqlserver.\n",
      "INSERT 6.26s: '100000 rows' successfully written to table 'fact_sales_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT 5.65s: '100000 rows' successfully written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_sales_gold' in sqlserver.\n",
      "INSERT 5.79s: '83564 rows' successfully written to table 'fact_sales_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT 3.29s: '100000 rows' successfully written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT 2.99s: '100000 rows' successfully written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT 3.17s: '100000 rows' successfully written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT 3.21s: '100000 rows' successfully written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT 3.19s: '100000 rows' successfully written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT 3.08s: '100000 rows' successfully written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT 3.30s: '100000 rows' successfully written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT 3.29s: '100000 rows' successfully written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT 4.46s: '100000 rows' successfully written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT 3.39s: '100000 rows' successfully written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT 3.73s: '100000 rows' successfully written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT 3.92s: '100000 rows' successfully written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT 3.17s: '100000 rows' successfully written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT 3.19s: '100000 rows' successfully written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT 3.36s: '100000 rows' successfully written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT 3.13s: '100000 rows' successfully written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT 3.30s: '100000 rows' successfully written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT 3.56s: '100000 rows' successfully written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT 4.04s: '100000 rows' successfully written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT 4.15s: '100000 rows' successfully written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT 3.43s: '100000 rows' successfully written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT 3.17s: '100000 rows' successfully written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT: Chunk of data written to table 'fact_gl_gold' in sqlserver.\n",
      "INSERT 2.29s: '70372 rows' successfully written to table 'fact_gl_gold' in sqlserver.\n",
      "All operations completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import duckdb\n",
    "import polars as pl\n",
    "import turbodbc\n",
    "import pyodbc\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import logging\n",
    "import time  # For measuring insert time\n",
    "from typing import Union\n",
    "\n",
    "# Initialize a list to collect error messages\n",
    "error_messages = []\n",
    "\n",
    "def get_database_connection(destination_database_system: str, connection_details: Union[str, dict]):\n",
    "    try:\n",
    "        if isinstance(connection_details, str):\n",
    "            return turbodbc.connect(dsn=connection_details)\n",
    "        \n",
    "        if not isinstance(connection_details, dict):\n",
    "            raise ValueError(f\"Connection details must be a dictionary or a string (DSN). Got: {type(connection_details)}\")\n",
    "\n",
    "        if destination_database_system == 'sqlserver':\n",
    "            return turbodbc.connect(\n",
    "                DRIVER=connection_details['driver'],\n",
    "                SERVER=connection_details['server'],\n",
    "                UID=connection_details['user'],\n",
    "                PWD=connection_details['password'],\n",
    "                DATABASE=connection_details['database']\n",
    "            )\n",
    "        elif destination_database_system == 'mysql':\n",
    "            return turbodbc.connect(\n",
    "                host=connection_details['server'],\n",
    "                user=connection_details['user'],\n",
    "                password=connection_details['password'],\n",
    "                database=connection_details['database']\n",
    "            )\n",
    "        elif destination_database_system == 'postgres':\n",
    "            return turbodbc.connect(\n",
    "                host=connection_details['server'],\n",
    "                user=connection_details['user'],\n",
    "                password=connection_details['password'],\n",
    "                dbname=connection_details['database']\n",
    "            )\n",
    "        elif destination_database_system == 'duckdb':\n",
    "            return duckdb.connect(connection_details['database'])\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported destination database system: {destination_database_system}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to connect to {destination_database_system}: {e}\")\n",
    "        raise\n",
    "\n",
    "def execute_non_transactional_query(query, connection_string):\n",
    "    try:\n",
    "        with pyodbc.connect(connection_string) as conn:\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(query)\n",
    "            print(f\"QUERY: {query}\")\n",
    "    except Exception as e:\n",
    "        error_messages.append(f\"Error executing query '{query}': {e}\")\n",
    "\n",
    "def ensure_table_exists(df_arrow, table_name, connection_string, destination_database_system):\n",
    "    column_definitions = \", \".join([f\"[{col.name}] NVARCHAR(MAX)\" for col in df_arrow.schema])\n",
    "    check_table_query = (\n",
    "        f\"IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = '{table_name}') \"\n",
    "        f\"CREATE TABLE {table_name} ({column_definitions})\"\n",
    "    )\n",
    "    execute_non_transactional_query(check_table_query, connection_string)\n",
    "    logging.info(f\"TABLE: '{table_name}' checked/created successfully in {destination_database_system}.\")\n",
    "\n",
    "def convert_chunked_arrays_to_numpy(df_arrow):\n",
    "    \"\"\"Convert Arrow Table columns to Numpy arrays.\"\"\"\n",
    "    return [column.to_numpy() for column in df_arrow.columns]\n",
    "\n",
    "def process_chunk(chunk, table_name, connection_string, destination_database_system, use_convert_to_text):\n",
    "    num_columns = len(chunk.schema)\n",
    "    placeholders = \", \".join([\"?\"] * num_columns)\n",
    "    insert_query = f\"INSERT INTO {table_name} VALUES ({placeholders})\"\n",
    "\n",
    "    with turbodbc.connect(connection_string=connection_string) as conn:\n",
    "        with conn.cursor() as cursor:\n",
    "            if not use_convert_to_text:\n",
    "                cursor.executemanycolumns(insert_query, chunk)\n",
    "            else:\n",
    "                numpy_columns = convert_chunked_arrays_to_numpy(chunk)\n",
    "                cursor.executemanycolumns(insert_query, numpy_columns)\n",
    "            conn.commit()\n",
    "            logging.info(f\"INSERT: Chunk of data written to table '{table_name}' in {destination_database_system}.\")\n",
    "\n",
    "def process_table(table, duckdb_connection, connection_string, chunk_size, destination_database_system):\n",
    "    con_duckdb = duckdb.connect(duckdb_connection)\n",
    "    try:\n",
    "        df_arrow = con_duckdb.execute(f'SELECT * FROM {table}').fetch_arrow_table()\n",
    "\n",
    "        if df_arrow.num_rows == 0:\n",
    "            logging.info(f\"Skipping empty table {table}.\")\n",
    "            return\n",
    "\n",
    "        ensure_table_exists(df_arrow, table, connection_string, destination_database_system)\n",
    "\n",
    "        num_rows = df_arrow.num_rows\n",
    "        if num_rows > chunk_size:\n",
    "            num_chunks = (num_rows // chunk_size) + (1 if num_rows % chunk_size > 0 else 0)\n",
    "            logging.info(f\"Processing large table '{table}' in {num_chunks} chunks.\")\n",
    "            \n",
    "            for i in range(num_chunks):\n",
    "                chunk_start = i * chunk_size\n",
    "                chunk_end = min(chunk_start + chunk_size, num_rows)\n",
    "                chunk = df_arrow.slice(chunk_start, chunk_end - chunk_start)\n",
    "\n",
    "                start_time = time.time()\n",
    "                process_chunk(chunk, table, connection_string, destination_database_system, use_convert_to_text=False)\n",
    "                end_time = time.time()\n",
    "                process_time = end_time - start_time\n",
    "                logging.info(f\"INSERT {process_time:.2f}s: '{chunk.num_rows} rows' successfully written to table '{table}' in {destination_database_system}.\")\n",
    "\n",
    "        else:\n",
    "            # For smaller tables, use chunked arrays for faster bulk insert\n",
    "            columns = \", \".join([f\"[{col.name}]\" for col in df_arrow.schema])\n",
    "            placeholders = \", \".join([\"?\"] * len(df_arrow.schema))\n",
    "            insert_query = f\"INSERT INTO {table} ({columns}) VALUES ({placeholders})\"\n",
    "\n",
    "            with turbodbc.connect(connection_string=connection_string) as conn:\n",
    "                cursor = conn.cursor()\n",
    "                start_time = time.time()\n",
    "                cursor.executemanycolumns(insert_query, df_arrow)\n",
    "                conn.commit()\n",
    "                end_time = time.time()\n",
    "                process_time = end_time - start_time\n",
    "                logging.info(f\"INSERT {process_time:.2f}s: '{df_arrow.num_rows} rows' successfully written to table '{table}' in {destination_database_system}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        error_messages.append(f\"Error while processing table '{table}': {e}\")\n",
    "    finally:\n",
    "        con_duckdb.close()\n",
    "\n",
    "def write_to_local_database(\n",
    "    source_database_connection: str,\n",
    "    destination_connection_string: Union[str, dict],\n",
    "    source_database_system: str = 'duckdb',\n",
    "    destination_database_system: str = 'sqlserver',\n",
    "    destination_database_name: str = None,\n",
    "    mode: str = 'overwrite',\n",
    "    convert_to_text: bool = False,\n",
    "    chunk_size: int = 100000  # Default chunk size for large tables\n",
    "):\n",
    "    logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "\n",
    "    supported_destination_systems = ['sqlserver', 'mysql', 'postgres', 'interbase', 'duckdb']\n",
    "\n",
    "    if source_database_system not in ['duckdb']:\n",
    "        logging.error(f\"Unsupported source database system: {source_database_system}\")\n",
    "        raise ValueError(\"Supported source database systems: 'duckdb'\")\n",
    "    if destination_database_system not in supported_destination_systems:\n",
    "        logging.error(f\"Unsupported destination database system: {destination_database_system}\")\n",
    "        raise ValueError(f\"Supported destination database systems: {supported_destination_systems}\")\n",
    "\n",
    "    if isinstance(destination_connection_string, dict):\n",
    "        required_keys = ['server', 'user', 'password', 'database']\n",
    "        if not all(key in destination_connection_string for key in required_keys):\n",
    "            logging.error(\"Missing required connection parameters.\")\n",
    "            raise ValueError(\"Ensure server, user, password, and database are set in the connection string.\")\n",
    "        conn_string = (\n",
    "            f\"DRIVER={destination_connection_string['driver']};\"\n",
    "            f\"SERVER={destination_connection_string['server']};\"\n",
    "            f\"UID={destination_connection_string['user']};\"\n",
    "            f\"PWD={destination_connection_string['password']};\"\n",
    "            f\"DATABASE={destination_connection_string['database']}\"\n",
    "        )\n",
    "    elif isinstance(destination_connection_string, str):\n",
    "        conn_string = destination_connection_string\n",
    "\n",
    "    def check_database_exists():\n",
    "        if destination_database_system == 'sqlserver':\n",
    "            database_name = destination_database_name or destination_connection_string['database']\n",
    "\n",
    "            check_db_query = f\"SELECT name FROM sys.databases WHERE name = '{database_name}'\"\n",
    "            try:\n",
    "                with get_database_connection(destination_database_system, destination_connection_string) as conn:\n",
    "                    cursor = conn.cursor()\n",
    "                    cursor.execute(check_db_query)\n",
    "                    db_exists = cursor.fetchone()\n",
    "                    if not db_exists:\n",
    "                        logging.error(f\"DATABASE: '{database_name}' does not exist in SQL Server. Create it to proceed.\")\n",
    "                        return False\n",
    "                    logging.info(f\"DATABASE: '{database_name}' exists.\")\n",
    "                    return True\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error checking if database exists: {e}\")\n",
    "                return False\n",
    "        else:\n",
    "            logging.info(f\"Skipping database existence check for {destination_database_system}\")\n",
    "            return True\n",
    "\n",
    "    if not check_database_exists():\n",
    "        raise SystemExit(f\"Cannot proceed: Database '{destination_database_name or destination_connection_string['database']}' does not exist in {destination_database_system}.\")\n",
    "\n",
    "    con_duckdb = duckdb.connect(source_database_connection)\n",
    "    try:\n",
    "        tables = con_duckdb.execute(\"SHOW TABLES\").fetchdf()\n",
    "        table_list = tables['name'].tolist()\n",
    "    finally:\n",
    "        con_duckdb.close()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                process_table,\n",
    "                table,\n",
    "                source_database_connection,\n",
    "                conn_string,\n",
    "                chunk_size,\n",
    "                destination_database_system\n",
    "            )\n",
    "            for table in table_list\n",
    "        ]\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                error_messages.append(f\"Error occurred during parallel execution: {e}\")\n",
    "\n",
    "    logging.info(\"All operations completed successfully.\")\n",
    "\n",
    "    if error_messages:\n",
    "        print(\"\\nErrors encountered during execution:\")\n",
    "        for message in error_messages:\n",
    "            print(message)\n",
    "\n",
    "# Example usage of the write_to_local_database function\n",
    "source_database_connection = '../Tables/LH_Gold.db'\n",
    "\n",
    "# DSN Connection example\n",
    "dsn_connection = 'sqlserver-odbc'  # DSN Connection\n",
    "destination_database_name = 'LH_GOLD'  # Must specify the database name when using DSN\n",
    "\n",
    "# Direct Connection String example\n",
    "destination_connection_string = {\n",
    "    \"driver\": \"{ODBC Driver 17 for SQL Server}\",\n",
    "    \"server\": os.getenv(\"server\"),\n",
    "    \"user\": os.getenv(\"user\"),\n",
    "    \"password\": os.getenv(\"password\"),\n",
    "    \"database\": \"LH_GOLD\"\n",
    "}\n",
    "\n",
    "# # Use DSN Connection\n",
    "# write_to_local_database(\n",
    "#     source_database_system='duckdb',\n",
    "#     source_database_connection=source_database_connection,\n",
    "#     destination_database_system='sqlserver',\n",
    "#     destination_connection_string=dsn_connection,\n",
    "#     destination_database_name=destination_database_name,  # Required when using DSN\n",
    "#     mode='append',         # (Optional): Control whether to overwrite or append data in database. Options: 'overwrite' or 'append' (default is overwrite).\n",
    "#     convert_to_text=False  # (Optional): Control whether to convert all columns to text. Options: True or False (default is True). If False, retry with True if an error occurs.\n",
    "# )\n",
    "\n",
    "# Use Direct Connection Parameters\n",
    "write_to_local_database(\n",
    "    source_database_system='duckdb',\n",
    "    source_database_connection=source_database_connection,\n",
    "    destination_database_system='sqlserver',\n",
    "    destination_connection_string=destination_connection_string,  # Direct connection parameters\n",
    "    mode='append',         # (Optional): Control whether to overwrite or append data in database. Options: 'overwrite' or 'append' (default is overwrite).\n",
    "    convert_to_text=False  # (Optional): Control whether to convert all columns to text. Options: True or False (default is True). If False, retry with True if an error occurs.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<u>MySQL</u>**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
