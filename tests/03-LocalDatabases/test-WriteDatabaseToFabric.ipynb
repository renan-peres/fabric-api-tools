{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Install/Update Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SQL Server (ODBC/DSN or URI)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run func-WriteDatabaseToFabric.py\n",
    "import pymssql\n",
    "import connectorx\n",
    "\n",
    "\"\"\"\n",
    "write_database_to_fabric(): Writes tables from Local Database (SQLServer, MySQL, PostgreSQL, InterBase, DuckDB) as a Delta table in Fabric.\n",
    "    Parameters:\n",
    "    - file_system_client:  (Required): Azure Data Lake Storage (ADLS) system client for handling file operations.\n",
    "    - database_system:     (Required): Database system being used. Options: \"sqlserver\", \"mysql\", \"postgres\", \"interbase\", \"duckdb\".\n",
    "    - database_connection: (Required): Database connection object. Options: connection string dictionary or DSN name for the ODBC driver.\n",
    "    - lakehouse_path:      (Required): Path in the Lakehouse where Delta tables will be stored. Options: \"Tables/\" or \"Files/\".\n",
    "    - database_name:       (Optional): Name of the database. If None, all databases will be considered.\n",
    "    - table_name:          (Optional): Table name (str), list of table names, or tables that contain (str) to be processed. If None, all tables will be processed.\n",
    "    - table_type:          (Optional): Filter the table type. Options: \"table\" or \"view\". \n",
    "    - convert_to_text:     (Optional): Control whether to convert all columns to text. Options: True or False (default is False).\n",
    "    - clean_column_names:  (Optional): Control whether to clean column names. Options: True or False (default is False).\n",
    "    - case_type:           (Optional): Case conversion for column names, either \"lower\", \"upper\", or \"proper\" (default is \"lower\").\n",
    "    - limit_rows:          (Optional): Limit the number of rows fetched from each table. If None, all rows will be processed.\n",
    "    - use_uri:             (Optional): Use URI method for connection. Options: True (uses ConnectorX) or False (uses ODBC/DSN). Default is False.\n",
    "\"\"\"\n",
    "\n",
    "# Get Azure Authentication Token\n",
    "token = get_authentication_token()\n",
    "file_system_client = get_file_system_client(token)\n",
    "\n",
    "# Database Connection Dictionary\n",
    "connection_string = {\n",
    "    \"server\": os.getenv(\"server\"),\n",
    "    \"user\": os.getenv(\"user\"),\n",
    "    \"password\": os.getenv(\"password\"),\n",
    "    \"database\": os.getenv(\"database\")\n",
    "}\n",
    "dsn = 'sqlserver-odbc'\n",
    "\n",
    "# Write Database to Lakehouse\n",
    "write_database_to_fabric(\n",
    "    file_system_client=file_system_client,\n",
    "    database_system='sqlserver',  # Database System ('sqlserver', 'mysql', 'postgres', 'interbase', 'duckdb')\n",
    "    database_connection=connection_string,  # Connection dictionary\n",
    "    lakehouse_path='Tables/',  # Lakehouse Tables\n",
    "    # lakehouse_path='Files/SQLServer/',  # Lakehouse Files\n",
    "    # database_name='AdventureWorksDW2020',\n",
    "    # table_name='DimAccount',  # Single Table | ['DimAccount', 'DimDate', 'DimCurrency'] # List of Tables | 'dim' Approximate match\n",
    "    table_type='table',  # (Optional): Filter the table type. Options: \"table\" or \"view\".\n",
    "    convert_to_text=True,  # (Optional): Control whether to convert all columns to text. Options: True or False (default is False).\n",
    "    clean_column_names=True,  # (Optional): Control whether to clean column names. Options: True or False (default is False).\n",
    "    case_type=\"upper\",  # (Optional): Case conversion for column names, either \"lower\", \"upper\", or \"proper\" (default is \"lower\").\n",
    "    # limit_rows=100,  # (Optional): Limit the number of rows fetched from each table. If None, all rows will be processed.\n",
    "    # use_uri=False  # Use ODBC/DSN method\n",
    "    use_uri=True  # Use URI method (faster, uses ConnectorX)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MySQL (ODBC/DSN or URI)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run func-WriteDatabaseToFabric.py\n",
    "import pyodbc\n",
    "import connectorx\n",
    "\n",
    "\"\"\"\n",
    "write_database_to_fabric(): Writes tables from Local Database (SQLServer, MySQL, PostgreSQL, InterBase, DuckDB) as a Delta table in Fabric.\n",
    "    Parameters:\n",
    "    - file_system_client:  (Required): Azure Data Lake Storage (ADLS) system client for handling file operations.\n",
    "    - database_system:     (Required): Database system being used. Options: \"sqlserver\", \"mysql\", \"postgres\", \"interbase\", \"duckdb\".\n",
    "    - database_connection: (Required): Database connection object. Options: connection string dictionary or DSN name for the ODBC driver.\n",
    "    - lakehouse_path:      (Required): Path in the Lakehouse where Delta tables will be stored. Options: \"Tables/\" or \"Files/\".\n",
    "    - database_name:       (Optional): Name of the database. If None, all databases will be considered.\n",
    "    - table_name:          (Optional): Table name (str), list of table names, or tables that contain (str) to be processed. If None, all tables will be processed.\n",
    "    - table_type:          (Optional): Filter the table type. Options: \"table\" or \"view\". \n",
    "    - convert_to_text:     (Optional): Control whether to convert all columns to text. Options: True or False (default is False).\n",
    "    - clean_column_names:  (Optional): Control whether to clean column names. Options: True or False (default is False).\n",
    "    - case_type:           (Optional): Case conversion for column names, either \"lower\", \"upper\", or \"proper\" (default is \"lower\").\n",
    "    - limit_rows:          (Optional): Limit the number of rows fetched from each table. If None, all rows will be processed.\n",
    "    - use_uri:             (Optional): Use URI method for connection. Options: True (uses ConnectorX) or False (uses ODBC/DSN). Default is False.\n",
    "\"\"\"\n",
    "\n",
    "# Get Azure Authentication Token\n",
    "token = get_authentication_token()\n",
    "file_system_client = get_file_system_client(token)\n",
    "\n",
    "# Database Connection Dictionary\n",
    "connection_string = {\n",
    "    \"server\": os.getenv(\"mysql_server\"),\n",
    "    \"user\": os.getenv(\"mysql_user\"),\n",
    "    \"password\": os.getenv(\"mysql_password\")\n",
    "}\n",
    "dsn = 'mysql-odbc'\n",
    "\n",
    "# Write Database to Lakehouse\n",
    "write_database_to_fabric(\n",
    "    file_system_client=file_system_client\n",
    "    ,database_system='mysql'  # Database System ('sqlserver', 'mysql', 'postgres', 'interbase', 'duckdb')\n",
    "    ,database_connection=connection_string  # Connection dictionary\n",
    "    ,lakehouse_path = 'Tables/' # Lakehouse Tables\n",
    "    # ,lakehouse_path = 'Files/MySQL/' # Lakehouse Files\n",
    "    # ,database_name = 'world' \n",
    "    # ,table_name = 'city' # Single Table | ['city', 'country'] # List of Tables | 'dim' Approximate match\n",
    "    ,table_type='view'  # (Optional): Filter the table type. Options: \"table\" or \"view\".\n",
    "    ,convert_to_text=True  # (Optional): Control whether to convert all columns to text. Options: True or False (default is False).\n",
    "    ,clean_column_names=True  # (Optional): Control whether to clean column names. Options: True or False (default is False).\n",
    "    ,case_type=\"upper\"  # (Optional): Case conversion for column names, either \"lower\", \"upper\", or \"proper\" (default is \"lower\").\n",
    "    # ,limit_rows=100,  # (Optional): Limit the number of rows fetched from each table. If None, all rows will be processed.\n",
    "    # ,use_uri=False  # Use ODBC/DSN method\n",
    "    ,use_uri=True  # Use URI method (faster, uses ConnectorX)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PostgreSQL (ODBC/DSN or URI)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run func-WriteDatabaseToFabric.py\n",
    "import pyodbc\n",
    "import connectorx\n",
    "\n",
    "\"\"\"\n",
    "write_database_to_fabric(): Writes tables from Local Database (SQLServer, MySQL, PostgreSQL, InterBase, DuckDB) as a Delta table in Fabric.\n",
    "    Parameters:\n",
    "    - file_system_client:  (Required): Azure Data Lake Storage (ADLS) system client for handling file operations.\n",
    "    - database_system:     (Required): Database system being used. Options: \"sqlserver\", \"mysql\", \"postgres\", \"interbase\", \"duckdb\".\n",
    "    - database_connection: (Required): Database connection object. Options: connection string dictionary or DSN name for the ODBC driver.\n",
    "    - lakehouse_path:      (Required): Path in the Lakehouse where Delta tables will be stored. Options: \"Tables/\" or \"Files/\".\n",
    "    - database_name:       (Optional): Name of the database. If None, all databases will be considered.\n",
    "    - table_name:          (Optional): Table name (str), list of table names, or tables that contain (str) to be processed. If None, all tables will be processed.\n",
    "    - table_type:          (Optional): Filter the table type. Options: \"table\" or \"view\". \n",
    "    - convert_to_text:     (Optional): Control whether to convert all columns to text. Options: True or False (default is False).\n",
    "    - clean_column_names:  (Optional): Control whether to clean column names. Options: True or False (default is False).\n",
    "    - case_type:           (Optional): Case conversion for column names, either \"lower\", \"upper\", or \"proper\" (default is \"lower\").\n",
    "    - limit_rows:          (Optional): Limit the number of rows fetched from each table. If None, all rows will be processed.\n",
    "    - use_uri:             (Optional): Use URI method for connection. Options: True (uses ConnectorX) or False (uses ODBC/DSN). Default is False.\n",
    "\"\"\"\n",
    "\n",
    "# Get Azure Authentication Token\n",
    "token = get_authentication_token()\n",
    "file_system_client = get_file_system_client(token)\n",
    "\n",
    "# Database Connection Dictionary\n",
    "connection_string = {\n",
    "    \"server\": os.getenv(\"pg_server\"),\n",
    "    \"user\": os.getenv(\"pg_username\"),\n",
    "    \"password\": os.getenv(\"pg_password\"),\n",
    "    \"port\": os.getenv(\"pg_port\"),\n",
    "    \"database\": os.getenv(\"pg_database\")    \n",
    "}\n",
    "dsn = 'postgres-odbc'\n",
    "\n",
    "# Write Database to Lakehouse\n",
    "write_database_to_fabric(\n",
    "    file_system_client=file_system_client\n",
    "    ,database_system='postgres'  # Database System ('sqlserver', 'mysql', 'postgres', 'interbase', 'duckdb')\n",
    "    ,database_connection=connection_string  # Connection dictionary\n",
    "    ,lakehouse_path = 'Tables/' # Lakehouse Tables\n",
    "    # ,lakehouse_path = 'Files/PostgreSQL/' # Lakehouse Files\n",
    "    # ,database_name = 'postgres' \n",
    "    # ,table_name = 'city' # Single Table | ['city', 'country'] # List of Tables | 'dim' Approximate match\n",
    "    # ,table_type= 'view'   # (Optional): Filter the table type. Options: \"table\" or \"view\". \n",
    "    ,convert_to_text=True  # (Optional): Control whether to convert all columns to text. Options: True or False (default is False).\n",
    "    ,clean_column_names=True  # (Optional): Control whether to clean column names. Options: True or False (default is False).\n",
    "    ,case_type=\"upper\"  # (Optional): Case conversion for column names, either \"lower\", \"upper\", or \"proper\" (default is \"lower\").\n",
    "    # ,limit_rows=100,  # (Optional): Limit the number of rows fetched from each table. If None, all rows will be processed.\n",
    "    # ,use_uri=False  # Use ODBC/DSN method\n",
    "    ,use_uri=True  # Use URI method (faster, uses ConnectorX)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **InterBase (ODBC/DSN Only)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run func-WriteDatabaseToFabric.py\n",
    "import pyodbc\n",
    "\n",
    "\"\"\"\n",
    "write_database_to_fabric(): Writes tables from Local Database (SQLServer, MySQL, PostgreSQL, InterBase, DuckDB) as a Delta table in Fabric.\n",
    "    Parameters:\n",
    "    - file_system_client:  (Required): Azure Data Lake Storage (ADLS) system client for handling file operations.\n",
    "    - database_system:     (Required): Database system being used. Options: \"sqlserver\", \"mysql\", \"postgres\", \"interbase\", \"duckdb\".\n",
    "    - database_connection: (Required): Database connection object. Options: connection string dictionary or DSN name for the ODBC driver.\n",
    "    - lakehouse_path:      (Required): Path in the Lakehouse where Delta tables will be stored. Options: \"Tables/\" or \"Files/\".\n",
    "    - database_name:       (Optional): Name of the database. If None, all databases will be considered.\n",
    "    - table_name:          (Optional): Table name (str), list of table names, or tables that contain (str) to be processed. If None, all tables will be processed.\n",
    "    - table_type:          (Optional): Filter the table type. Options: \"table\" or \"view\". \n",
    "    - convert_to_text:     (Optional): Control whether to convert all columns to text. Options: True or False (default is False).\n",
    "    - clean_column_names:  (Optional): Control whether to clean column names. Options: True or False (default is False).\n",
    "    - case_type:           (Optional): Case conversion for column names, either \"lower\", \"upper\", or \"proper\" (default is \"lower\").\n",
    "    - limit_rows:          (Optional): Limit the number of rows fetched from each table. If None, all rows will be processed.\n",
    "    - use_uri:             (Optional): Use URI method for connection. Options: True (uses ConnectorX) or False (uses ODBC/DSN). Default is False.\n",
    "\"\"\"\n",
    "\n",
    "# Get Azure Authentication Token\n",
    "token = get_authentication_token()\n",
    "file_system_client = get_file_system_client(token)\n",
    "\n",
    "# Write Database to Lakehouse (ODBC Only)\n",
    "write_database_to_fabric(\n",
    "    file_system_client=file_system_client\n",
    "    ,database_system='interbase'  # Database System ('sqlserver', 'mysql', 'postgres, 'interbase', 'duckdb')\n",
    "    ,database_connection=\"interbase-odbc\" # Connection Via DSN\n",
    "    ,lakehouse_path = 'Tables/' # Lakehouse Tables\n",
    "    # ,lakehouse_path = 'Files/InterBase/' # Lakehouse Files\n",
    "    # ,database_name = 'CCW001' \n",
    "    # ,table_name = 'CCTEVENT' # Single Table | ['CCTEVENT', 'GLTACCT'] # List of Tables | 'dim' Approximate match\n",
    "    ,table_type= 'view'   # (Optional): Filter the table type. Options: \"table\" or \"view\". \n",
    "    ,convert_to_text=True  # (Optional): Control whether to convert all columns to text. Options: True or False (default is False).\n",
    "    ,clean_column_names=True  # (Optional): Control whether to clean column names. Options: True or False (default is False).\n",
    "    ,case_type=\"upper\"  # (Optional): Case conversion for column names, either \"lower\", \"upper\", or \"proper\" (default is \"lower\").\n",
    "    # ,limit_rows=100,  # (Optional): Limit the number of rows fetched from each table. If None, all rows will be processed.\n",
    "    ,use_uri=False  # Use ODBC/DSN method\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DuckDB (Local/MotherDuck)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run func-WriteDatabaseToFabric.py\n",
    "import duckdb\n",
    "\n",
    "\"\"\"\n",
    "write_database_to_fabric(): Writes tables from Local Database (SQLServer, MySQL, PostgreSQL, InterBase, DuckDB) as a Delta table in Fabric.\n",
    "    Parameters:\n",
    "    - file_system_client:  (Required): Azure Data Lake Storage (ADLS) system client for handling file operations.\n",
    "    - database_system:     (Required): Database system being used. Options: \"sqlserver\", \"mysql\", \"postgres\", \"interbase\", \"duckdb\".\n",
    "    - database_connection: (Required): Database connection object. Options: connection string dictionary or DSN name for the ODBC driver.\n",
    "    - lakehouse_path:      (Required): Path in the Lakehouse where Delta tables will be stored. Options: \"Tables/\" or \"Files/\".\n",
    "    - database_name:       (Optional): Name of the database. If None, all databases will be considered.\n",
    "    - table_name:          (Optional): Table name (str), list of table names, or tables that contain (str) to be processed. If None, all tables will be processed.\n",
    "    - table_type:          (Optional): Filter the table type. Options: \"table\" or \"view\". \n",
    "    - convert_to_text:     (Optional): Control whether to convert all columns to text. Options: True or False (default is False).\n",
    "    - clean_column_names:  (Optional): Control whether to clean column names. Options: True or False (default is False).\n",
    "    - case_type:           (Optional): Case conversion for column names, either \"lower\", \"upper\", or \"proper\" (default is \"lower\").\n",
    "    - limit_rows:          (Optional): Limit the number of rows fetched from each table. If None, all rows will be processed.\n",
    "    - use_uri:             (Optional): Use URI method for connection. Options: True (uses ConnectorX) or False (uses ODBC/DSN). Default is False.\n",
    "\"\"\"\n",
    "\n",
    "# Get Azure Authentication Token\n",
    "token = get_authentication_token()\n",
    "file_system_client = get_file_system_client(token)\n",
    "\n",
    "# Write Database to Lakehouse (Local DuckDB File or Motherduck)\n",
    "write_database_to_fabric(\n",
    "    file_system_client=file_system_client\n",
    "    ,database_system='duckdb'  # Database System ('sqlserver', 'mysql', 'postgres, 'interbase', 'duckdb')\n",
    "    # ,database_connection=\"Tables/data.db\" # Connection Via Local Database\n",
    "    ,database_connection='md:' # Connection Via MotherDuck\n",
    "    ,lakehouse_path = 'Tables/' # Lakehouse Tables\n",
    "    # ,lakehouse_path = 'Files/DuckDB/' # Lakehouse Files\n",
    "    # ,database_name = 'LH_gold_remote' \n",
    "    # ,table_name = 'fact_gl_gold' # Single Table | ['fact_gl_gold', 'fact_sales_gold'] # List of Tables | 'dim' Approximate match\n",
    "    # ,table_type= 'view'   # (Optional): Filter the table type. Options: \"table\" or \"view\". \n",
    "    ,convert_to_text=True  # (Optional): Control whether to convert all columns to text. Options: True or False (default is False).\n",
    "    ,clean_column_names=True  # (Optional): Control whether to clean column names. Options: True or False (default is False).\n",
    "    ,case_type=\"upper\"  # (Optional): Case conversion for column names, either \"lower\", \"upper\", or \"proper\" (default is \"lower\").\n",
    "    # ,limit_rows=100,  # (Optional): Limit the number of rows fetched from each table. If None, all rows will be processed.\n",
    "    ,use_uri=False \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
