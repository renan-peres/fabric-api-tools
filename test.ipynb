{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Run Delta Table Maintenance (REST API)**\n",
    "**Reference:** [Background Jobs - Run On Demand Table Maintenance](https://learn.microsoft.com/en-us/rest/api/fabric/lakehouse/background-jobs/run-on-demand-table-maintenance?tabs=HTTP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../main.py\n",
    "\n",
    "def trigger_table_maintenance_job(table_name: str, token: str) -> str:\n",
    "    endpoint = f\"https://api.fabric.microsoft.com/v1/workspaces/{WORKSPACE_ID}/lakehouses/{LAKEHOUSE_ID}/jobs/instances?jobType=TableMaintenance\"\n",
    "    payload = {\"executionData\": {\"tableName\": table_name, \"optimizeSettings\": {\"vOrder\": True}, \"vacuumSettings\": {\"retentionPeriod\": \"7:01:00:00\"}}}\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n",
    "    response = requests.post(endpoint, json=payload, headers=headers)\n",
    "    if response.status_code == 202:\n",
    "        return response.headers.get(\"Location\")\n",
    "    else:\n",
    "        print(f\"Failed to trigger table maintenance job. Status code: {response.status_code}, Response text: {response.text}\")\n",
    "        return None\n",
    "    \n",
    "def get_bearer_token() -> str:\n",
    "    return InteractiveBrowserCredential().get_token(\"https://api.fabric.microsoft.com/.default\").token\n",
    "\n",
    "def get_authentication_token() -> DefaultAzureCredential:\n",
    "    return DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Single Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../main.py\n",
    "\n",
    "# Get token\n",
    "token = get_bearer_token() # Interactive Browser\n",
    "# token = get_authentication_token().get_token(\"https://api.fabric.microsoft.com/.default\").token # Service Principal\n",
    "\n",
    "# Execute table maintenance job\n",
    "trigger_table_maintenance_job(table_name=\"dim_coa_gold\", token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **All Tables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../main.py\n",
    "import time\n",
    "\n",
    "# Get token\n",
    "token = get_bearer_token() # Interactive Browser\n",
    "# token = get_authentication_token().get_token(\"https://api.fabric.microsoft.com/.default\").token # Service Principal\n",
    "file_system_client = get_file_system_client(get_authentication_token())\n",
    "\n",
    "# Get the filtered subdirectory names for \"Tables\"\n",
    "filtered_tables = list_items(file_system_client=file_system_client, target_directory_path=\"Tables\")\n",
    "\n",
    "# Define batch size and delay between batches\n",
    "batch_size = 5\n",
    "batch_delay = 60  # in seconds\n",
    "\n",
    "# Iterate over the filtered tables in batches\n",
    "for i in range(0, len(filtered_tables), batch_size):\n",
    "    batch_tables = filtered_tables[i:i + batch_size]\n",
    "    for table_name in batch_tables:\n",
    "        try:\n",
    "            result = trigger_table_maintenance_job(table_name=table_name, token=token)\n",
    "            if result is not None:\n",
    "                print(f\"Table maintenance job triggered for table: {table_name}\")\n",
    "            else:\n",
    "                print(f\"Failed to trigger table maintenance job for table: {table_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred for table {table_name}: {e}\")\n",
    "    \n",
    "    # Delay between batches\n",
    "    if i + batch_size < len(filtered_tables):\n",
    "        print(f\"Waiting for {batch_delay} seconds before triggering the next batch...\")\n",
    "        time.sleep(batch_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Run Data Pipeline**\n",
    "**Reference:** [Microsoft Fabric data pipeline public REST API](https://learn.microsoft.com/en-us/fabric/data-factory/pipeline-rest-api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run get-bearer-token.py\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "\n",
    "def trigger_pipeline_job(workspace_id: str, pipeline_id: str, token: str) -> str:\n",
    "    endpoint = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}/items/{pipeline_id}/jobs/instances?jobType=Pipeline\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n",
    "    response = requests.post(endpoint, headers=headers)\n",
    "    if response.status_code == 202:\n",
    "        return response.headers.get(\"Location\")\n",
    "    else:\n",
    "        response.raise_for_status()\n",
    "\n",
    "# Microsoft Fabric Details from Environment Variables\n",
    "workspace_id=os.getenv(\"WORKSPACE_ID\")\n",
    "pipeline_id = os.getenv(\"PIPELINE_ID\")\n",
    "token = json.load(open(\"token_store.json\", \"r\"))[\"token\"] # Get token from JSON File\n",
    "\n",
    "trigger_pipeline_job(workspace_id = workspace_id, \n",
    "                     pipeline_id=pipeline_id, \n",
    "                     token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Run Spark Notebook**\n",
    "**Reference:** [Manage and Execute Notebooks in Fabric with APIs](https://learn.microsoft.com/en-us/fabric/data-engineering/notebook-public-api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run get-bearer-token.py\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "\n",
    "def run_notebook_job(artifact_id: str, workspace_id: str, lakehouse_name: str, lakehouse_id: str, token: str) -> str:\n",
    "    endpoint = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}/items/{artifact_id}/jobs/instances?jobType=RunNotebook\"\n",
    "    payload = {\n",
    "        \"executionData\": {\n",
    "                \"defaultLakehouse\": {\n",
    "                    \"name\": lakehouse_name,\n",
    "                    \"id\": lakehouse_id,\n",
    "                },\n",
    "                \"useStarterPool\": True\n",
    "            }\n",
    "        }\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n",
    "    response = requests.post(endpoint, json=payload, headers=headers)\n",
    "    if response.status_code == 202:\n",
    "        return response.headers.get(\"Location\")\n",
    "    else:\n",
    "        print(f\"Failed to trigger table maintenance job. Status code: {response.status_code}, Response text: {response.text}\")\n",
    "        return None\n",
    "\n",
    "# Microsoft Fabric Details from Environment Variables\n",
    "workspace_id=os.getenv(\"WORKSPACE_ID\")\n",
    "artifact_id = os.getenv(\"ARTIFACT_ID\")\n",
    "lakehouse_id=os.getenv(\"LAKEHOUSE_ID\")\n",
    "lakehouse_name = os.getenv(\"LAKEHOUSE_NAME\")\n",
    "token = json.load(open(\"token_store.json\", \"r\"))[\"token\"] # Get token from JSON File\n",
    "\n",
    "run_notebook_job(workspace_id = workspace_id, \n",
    "                 artifact_id=artifact_id, \n",
    "                 lakehouse_id = lakehouse_id, \n",
    "                 lakehouse_name=lakehouse_name, \n",
    "                 token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import/Create Notebook**\n",
    "**Reference:** [Manage and Execute Notebooks in Fabric with APIs](https://learn.microsoft.com/en-us/fabric/data-engineering/notebook-public-api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run main.py\n",
    "%run get-bearer-token.py\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import base64\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "onelake_ops = OneLakeRemoteOperations()\n",
    "\n",
    "# Load environment variables\n",
    "workspace_id = os.getenv(\"WORKSPACE_ID\")\n",
    "lakehouse_id = os.getenv(\"LAKEHOUSE_ID\")\n",
    "lakehouse_name = os.getenv(\"LAKEHOUSE_NAME\")\n",
    "\n",
    "def poll_notebook_creation(location_url: str, token: str, max_retries: int = 20, retry_interval: int = 15) -> dict:\n",
    "    \"\"\"\n",
    "    Poll the status of notebook creation.\n",
    "\n",
    "    Args:\n",
    "        location_url (str): The URL to poll for creation status.\n",
    "        token (str): The authentication token.\n",
    "        max_retries (int): Maximum number of polling attempts.\n",
    "        retry_interval (int): Time in seconds between polling attempts.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the success status and details of the creation process.\n",
    "    \"\"\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            poll_response = requests.get(location_url, headers=headers)\n",
    "            poll_response.raise_for_status()\n",
    "            \n",
    "            response_json = poll_response.json()\n",
    "            print(f\"Poll response: {response_json}\")\n",
    "            \n",
    "            status = response_json.get('status', '').lower()\n",
    "            \n",
    "            if status == 'succeeded':\n",
    "                return {\"success\": True, \"id\": response_json.get('resourceId'), \"details\": response_json}\n",
    "            elif status in ['failed', 'canceled']:\n",
    "                return {\"success\": False, \"details\": response_json}\n",
    "            else:\n",
    "                print(f\"Creation in progress. Status: {status}. Attempt {attempt + 1}/{max_retries}\")\n",
    "            \n",
    "            time.sleep(retry_interval)\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error during polling: {e}\")\n",
    "            time.sleep(retry_interval)\n",
    "    \n",
    "    return {\"success\": False, \"details\": \"Polling exceeded maximum retries\"}\n",
    "\n",
    "def download_file_from_github(repo_url: str, file_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Download a specific file from a GitHub repository.\n",
    "\n",
    "    Args:\n",
    "        repo_url (str): The URL of the GitHub repository.\n",
    "        file_path (str): The path to the file within the repository.\n",
    "\n",
    "    Returns:\n",
    "        dict: The content of the file as a JSON object.\n",
    "    \"\"\"\n",
    "    # Extract owner and repo name from URL\n",
    "    parts = repo_url.rstrip('/').split('/')\n",
    "    owner, repo = parts[-2], parts[-1]\n",
    "    if repo.endswith('.git'):\n",
    "        repo = repo[:-4]\n",
    "\n",
    "    # Construct the raw content URL\n",
    "    raw_url = f\"https://raw.githubusercontent.com/{owner}/{repo}/main/{file_path}\"\n",
    "\n",
    "    # Download the file\n",
    "    response = requests.get(raw_url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Parse the content as JSON\n",
    "    return json.loads(response.text)\n",
    "\n",
    "def import_notebook_to_fabric(upload_from: str, source_path: str,\n",
    "                              default_lakehouse_workspace_id: str = None,\n",
    "                              default_lakehouse_name: str = None,\n",
    "                              default_lakehouse: str = None,\n",
    "                              workspaceId: str = None,\n",
    "                              environmentId: str = None,\n",
    "                              file_path: str = None):\n",
    "    \"\"\"\n",
    "    Import a notebook to Microsoft Fabric.\n",
    "\n",
    "    Args:\n",
    "        upload_from (str): The source of the notebook ('local', 'lakehouse', or 'github').\n",
    "        source_path (str): The path to the notebook file or GitHub repository URL.\n",
    "        default_lakehouse_workspace_id (str, optional): The default lakehouse workspace ID.\n",
    "        default_lakehouse_name (str, optional): The default lakehouse name.\n",
    "        default_lakehouse (str, optional): The default lakehouse ID.\n",
    "        workspaceId (str, optional): The workspace ID.\n",
    "        environmentId (str, optional): The environment ID.\n",
    "        file_path (str, optional): The path to the file within the GitHub repository.\n",
    "\n",
    "    This function handles the entire process of importing a notebook, including:\n",
    "    - Loading the notebook content\n",
    "    - Updating the metadata\n",
    "    - Creating the notebook in Fabric\n",
    "    - Polling for the creation status\n",
    "    \"\"\"\n",
    "\n",
    "    notebook_name = f\"{upload_from}_{os.path.splitext(os.path.basename(source_path))[0]}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "    # Load notebook content\n",
    "    if upload_from == \"local\":\n",
    "        if os.path.exists(source_path):\n",
    "            with open(source_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                notebook_json = json.load(file)\n",
    "        else:\n",
    "            print(f\"Failed to locate the local notebook file: {source_path}\")\n",
    "            return\n",
    "    elif upload_from == \"lakehouse\":\n",
    "        token_credential = onelake_ops.get_authentication_token()\n",
    "        file_system_client = DataLakeServiceClient(\n",
    "            f\"https://onelake.dfs.fabric.microsoft.com\",\n",
    "            credential=token_credential\n",
    "        ).get_file_system_client(workspace_id)\n",
    "\n",
    "        temp_file_path = onelake_ops.download_from_lakehouse_temp(file_system_client, source_path, lakehouse_id)\n",
    "        if temp_file_path:\n",
    "            with open(temp_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                notebook_json = json.load(file)\n",
    "            os.unlink(temp_file_path)  # Delete the temporary file\n",
    "        else:\n",
    "            print(\"Failed to download the notebook file from lakehouse.\")\n",
    "            return\n",
    "    elif upload_from == \"github\":\n",
    "        if not file_path:\n",
    "            print(\"File path is required for GitHub upload.\")\n",
    "            return\n",
    "        try:\n",
    "            notebook_json = download_file_from_github(source_path, file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download the notebook file from GitHub: {str(e)}\")\n",
    "            return\n",
    "    else:\n",
    "        print(\"Invalid upload_from parameter. Use 'local', 'lakehouse', or 'github'.\")\n",
    "        return\n",
    "\n",
    "    # Create new metadata structure\n",
    "    new_metadata = {\n",
    "        \"language_info\": {\n",
    "            \"name\": \"python\"\n",
    "        },\n",
    "        \"trident\": {\n",
    "            \"environment\": {\n",
    "                \"environmentId\": environmentId or \"6524967a-18dc-44ae-86d1-0ec903e7ca05\",\n",
    "                \"workspaceId\": workspaceId or workspace_id\n",
    "            },\n",
    "            \"lakehouse\": {\n",
    "                \"default_lakehouse\": default_lakehouse or lakehouse_id,\n",
    "                \"default_lakehouse_name\": default_lakehouse_name or lakehouse_name,\n",
    "                \"default_lakehouse_workspace_id\": default_lakehouse_workspace_id or workspace_id\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Create new notebook structure\n",
    "    new_notebook = {\n",
    "        \"nbformat\": 4,\n",
    "        \"nbformat_minor\": 5,\n",
    "        \"cells\": notebook_json.get(\"cells\", []),\n",
    "        \"metadata\": new_metadata\n",
    "    }\n",
    "\n",
    "    print(f\"Updated notebook metadata: {json.dumps(new_notebook['metadata'], indent=2)}\")\n",
    "\n",
    "    # Convert to base64\n",
    "    base64_notebook_content = base64.b64encode(json.dumps(new_notebook).encode('utf-8')).decode('utf-8')\n",
    "\n",
    "    # Create notebook\n",
    "    endpoint = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}/items\"\n",
    "    payload = {\n",
    "        \"displayName\": notebook_name,\n",
    "        \"type\": \"Notebook\",\n",
    "        \"description\": \"Notebook created via API\",\n",
    "        \"definition\": {\n",
    "            \"format\": \"ipynb\",\n",
    "            \"parts\": [{\"path\": \"artifact.content.ipynb\", \"payload\": base64_notebook_content, \"payloadType\": \"InlineBase64\"}]\n",
    "        }\n",
    "    }\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    response = requests.post(endpoint, json=payload, headers=headers)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    if response.status_code == 201:\n",
    "        print(f\"Notebook created successfully. ID: {response.json()['id']}\")\n",
    "    elif response.status_code == 202:\n",
    "        location_url = response.headers.get(\"Location\")\n",
    "        print(\"Notebook creation in progress. Polling for status...\")\n",
    "        poll_result = poll_notebook_creation(location_url, token)\n",
    "        if poll_result[\"success\"]:\n",
    "            print(f\"Notebook created successfully. ID: {poll_result['id']}\")\n",
    "        else:\n",
    "            print(f\"Failed to create notebook. Details: {poll_result['details']}\")\n",
    "    else:\n",
    "        print(f\"Unexpected response. Status code: {response.status_code}, Response: {response.text}\")\n",
    "\n",
    "# Example usage\n",
    "import_notebook_to_fabric(\n",
    "    upload_from=\"github\", # ('local', 'lakehouse', or 'github')\n",
    "    source_path=\"https://github.com/renan-peres/Polars-Cookbook.git\",\n",
    "    file_path=\"Chapter03/ch03.ipynb\",\n",
    "    # Optional parameters:\n",
    "    # default_lakehouse_workspace_id=\"custom_workspace_id\",\n",
    "    # default_lakehouse_name=\"LH_bronze\",\n",
    "    # default_lakehouse=\"46389222-328e-4e65-aa06-02a380dd60d8\",\n",
    "    # workspaceId=\"custom_workspace_id\",\n",
    "    # environmentId=\"custom_environment_id\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
