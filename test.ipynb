{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import OneLakeRemoteOperations Module from GitHub**\n",
    "**Reference:** [Microsoft Fabric: OneLake Remote Operations](https://github.com/renan-peres/fabric-onelake-remote-operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Fetch the main.py file from the GitHub repository\n",
    "url = \"https://raw.githubusercontent.com/renan-peres/fabric-onelake-remote-operations/main/main.py\"\n",
    "response = requests.get(url)\n",
    "exec(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Assign OneLakeRemoteOperations and FabricAPIOperations to Variables, & Get Authentication Token**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run main.py\n",
    "\n",
    "# Initiate OneLakeRemoteOperations and get the token\n",
    "onelake_ops = OneLakeRemoteOperations()\n",
    "\n",
    "# Get Authentication token from OneLakeRemoteOperations and Save into 'token_store.json'\n",
    "token = onelake_ops.get_bearer_token()\n",
    "\n",
    "# Initiate FabricAPIOperations and call the method\n",
    "fabric_ops = FabricAPIOperations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import/Create Notebook**\n",
    "**Reference:** [Manage and Execute Notebooks in Fabric with APIs](https://learn.microsoft.com/en-us/fabric/data-engineering/notebook-public-api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Notebook in Workspace\n",
    "fabric_ops.import_notebook_to_fabric(\n",
    "    token=token,\n",
    "    upload_from=\"github\",  # ('local', 'lakehouse', or 'github')\n",
    "    # source_path=\"Files/GitHub/notebooks/R in Spark/R_in_PySpark_Notebooks.ipynb\", # lakehouse\n",
    "    source_path=\"https://github.com/renan-peres/Polars-Cookbook/blob/main/Chapter07/ch07.ipynb\", # github\n",
    "    # source_path=\"test.ipynb\", # local\n",
    "    known_lakehouses=[\"3b9aef1c-59f2-47d9-a8b2-d8b234536427\", \"46389222-328e-4e65-aa06-02a380dd60d8\"], # [LH_Test (Default), LH_bronze]\n",
    "    # Optional parameters:\n",
    "    # default_lakehouse_id=\"46389222-328e-4e65-aa06-02a380dd60d8\", # LH_bronze\n",
    "    # default_lakehouse_workspace_id=\"custom_workspace_id\",\n",
    "    # environment_id=\"custom_environment_id\",\n",
    "    # environment_workspace_id=\"custom_environmet_workspace_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Run Spark Notebook**\n",
    "**Reference:** [Manage and Execute Notebooks in Fabric with APIs](https://learn.microsoft.com/en-us/fabric/data-engineering/notebook-public-api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run get-bearer-token.py\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "\n",
    "def run_notebook_job(artifact_id: str, workspace_id: str, lakehouse_name: str, lakehouse_id: str, token: str) -> str:\n",
    "    endpoint = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}/items/{artifact_id}/jobs/instances?jobType=RunNotebook\"\n",
    "    payload = {\n",
    "        \"executionData\": {\n",
    "                \"defaultLakehouse\": {\n",
    "                    \"name\": lakehouse_name,\n",
    "                    \"id\": lakehouse_id,\n",
    "                },\n",
    "                \"useStarterPool\": True\n",
    "            }\n",
    "        }\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n",
    "    response = requests.post(endpoint, json=payload, headers=headers)\n",
    "    if response.status_code == 202:\n",
    "        return response.headers.get(\"Location\")\n",
    "    else:\n",
    "        print(f\"Failed to trigger table maintenance job. Status code: {response.status_code}, Response text: {response.text}\")\n",
    "        return None\n",
    "\n",
    "# Microsoft Fabric Details from Environment Variables\n",
    "workspace_id=os.getenv(\"WORKSPACE_ID\")\n",
    "artifact_id = os.getenv(\"ARTIFACT_ID\")\n",
    "lakehouse_id=os.getenv(\"LAKEHOUSE_ID\")\n",
    "lakehouse_name = os.getenv(\"LAKEHOUSE_NAME\")\n",
    "\n",
    "# Get the token using the class method\n",
    "onelake_ops = OneLakeRemoteOperations()\n",
    "token = onelake_ops.get_bearer_token()\n",
    "\n",
    "# Trigger the pipeline job\n",
    "run_notebook_job(workspace_id = workspace_id, \n",
    "                 artifact_id=artifact_id, \n",
    "                 lakehouse_id = lakehouse_id, \n",
    "                 lakehouse_name=lakehouse_name, \n",
    "                 token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Run Data Pipeline**\n",
    "**Reference:** [Microsoft Fabric data pipeline public REST API](https://learn.microsoft.com/en-us/fabric/data-factory/pipeline-rest-api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run main.py\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def trigger_pipeline_job(workspace_id: str, pipeline_id: str, token: str) -> str:\n",
    "    endpoint = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}/items/{pipeline_id}/jobs/instances?jobType=Pipeline\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n",
    "    response = requests.post(endpoint, headers=headers)\n",
    "    if response.status_code == 202:\n",
    "        return response.headers.get(\"Location\")\n",
    "    else:\n",
    "        response.raise_for_status()\n",
    "\n",
    "# Microsoft Fabric Details from Environment Variables\n",
    "workspace_id = os.getenv(\"WORKSPACE_ID\")\n",
    "pipeline_id = os.getenv(\"PIPELINE_ID\")\n",
    "\n",
    "# Get the token using the class method\n",
    "onelake_ops = OneLakeRemoteOperations()\n",
    "token = onelake_ops.get_bearer_token()\n",
    "\n",
    "# Trigger the pipeline job\n",
    "job_location = trigger_pipeline_job(workspace_id=workspace_id,\n",
    "                                    pipeline_id=pipeline_id,\n",
    "                                    token=token)\n",
    "\n",
    "print(f\"Pipeline job triggered. Job location: {job_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Run Delta Table Maintenance (REST API)**\n",
    "**Reference:** [Background Jobs - Run On Demand Table Maintenance](https://learn.microsoft.com/en-us/rest/api/fabric/lakehouse/background-jobs/run-on-demand-table-maintenance?tabs=HTTP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../main.py\n",
    "\n",
    "def trigger_table_maintenance_job(table_name: str, token: str) -> str:\n",
    "    endpoint = f\"https://api.fabric.microsoft.com/v1/workspaces/{WORKSPACE_ID}/lakehouses/{LAKEHOUSE_ID}/jobs/instances?jobType=TableMaintenance\"\n",
    "    payload = {\"executionData\": {\"tableName\": table_name, \"optimizeSettings\": {\"vOrder\": True}, \"vacuumSettings\": {\"retentionPeriod\": \"7:01:00:00\"}}}\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n",
    "    response = requests.post(endpoint, json=payload, headers=headers)\n",
    "    if response.status_code == 202:\n",
    "        return response.headers.get(\"Location\")\n",
    "    else:\n",
    "        print(f\"Failed to trigger table maintenance job. Status code: {response.status_code}, Response text: {response.text}\")\n",
    "        return None\n",
    "    \n",
    "def get_bearer_token() -> str:\n",
    "    return InteractiveBrowserCredential().get_token(\"https://api.fabric.microsoft.com/.default\").token\n",
    "\n",
    "def get_authentication_token() -> DefaultAzureCredential:\n",
    "    return DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Single Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../main.py\n",
    "\n",
    "# Get token\n",
    "token = get_bearer_token() # Interactive Browser\n",
    "# token = get_authentication_token().get_token(\"https://api.fabric.microsoft.com/.default\").token # Service Principal\n",
    "\n",
    "# Execute table maintenance job\n",
    "trigger_table_maintenance_job(table_name=\"dim_coa_gold\", token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **All Tables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../main.py\n",
    "import time\n",
    "\n",
    "# Get token\n",
    "token = get_bearer_token() # Interactive Browser\n",
    "# token = get_authentication_token().get_token(\"https://api.fabric.microsoft.com/.default\").token # Service Principal\n",
    "file_system_client = get_file_system_client(get_authentication_token())\n",
    "\n",
    "# Get the filtered subdirectory names for \"Tables\"\n",
    "filtered_tables = list_items(file_system_client=file_system_client, target_directory_path=\"Tables\")\n",
    "\n",
    "# Define batch size and delay between batches\n",
    "batch_size = 5\n",
    "batch_delay = 60  # in seconds\n",
    "\n",
    "# Iterate over the filtered tables in batches\n",
    "for i in range(0, len(filtered_tables), batch_size):\n",
    "    batch_tables = filtered_tables[i:i + batch_size]\n",
    "    for table_name in batch_tables:\n",
    "        try:\n",
    "            result = trigger_table_maintenance_job(table_name=table_name, token=token)\n",
    "            if result is not None:\n",
    "                print(f\"Table maintenance job triggered for table: {table_name}\")\n",
    "            else:\n",
    "                print(f\"Failed to trigger table maintenance job for table: {table_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred for table {table_name}: {e}\")\n",
    "    \n",
    "    # Delay between batches\n",
    "    if i + batch_size < len(filtered_tables):\n",
    "        print(f\"Waiting for {batch_delay} seconds before triggering the next batch...\")\n",
    "        time.sleep(batch_delay)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
